{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KUL H02A5a Computer Vision: Group Assignment 2\n",
    "---------------------------------------------------------------\n",
    "Student numbers: <span style=\"color:red\">r1, r2, r3, r4, r5</span>. (fill in your student numbers!)\n",
    "\n",
    "In this group assignment your team will delve into some deep learning applications for computer vision. The assignment will be delivered in the same groups from *Group assignment 1* and you start from this template notebook. The notebook you submit for grading is the last notebook pinned as default and submitted to the [Kaggle competition](https://www.kaggle.com/t/90a3b6380ecb4700857b9e07a44ca41b) prior to the deadline on **Tuesday 20 May 23:59**. Closely follow [these instructions](https://github.com/gourie/kaggle_inclass) for joining the competition, sharing your notebook with the TAs and making a valid notebook submission to the competition. A notebook submission not only produces a *submission.csv* file that is used to calculate your competition score, it also runs the entire notebook and saves its output as if it were a report. This way it becomes an all-in-one-place document for the TAs to review. As such, please make sure that your final submission notebook is self-contained and fully documented (e.g. provide strong arguments for the design choices that you make). Most likely, this notebook format is not appropriate to run all your experiments at submission time (e.g. the training of CNNs is a memory hungry and time consuming process; due to limited Kaggle resources). It can be a good idea to distribute your code otherwise and only summarize your findings, together with your final predictions, in the submission notebook. For example, you can substitute experiments with some text and figures that you have produced \"offline\" (e.g. learning curves and results on your internal validation set or even the test set for different architectures, pre-processing pipelines, etc). We advise you to first go through the PDF of this assignment entirely before you really start. Then, it can be a good idea to go through this notebook and use it as your first notebook submission to the competition. You can make use of the *Group assignment 2* forum/discussion board on Toledo if you have any questions. Good luck and have fun!\n",
    "\n",
    "---------------------------------------------------------------\n",
    "NOTES:\n",
    "* This notebook is just a template. Please keep the five main sections, but feel free to adjust further in any way you please!\n",
    "* Clearly indicate the improvements that you make! You can for instance use subsections like: *3.1. Improvement: applying loss function f instead of g*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview\n",
    "This assignment consists of *three main parts* for which we expect you to provide code and extensive documentation in the notebook:\n",
    "* Image classification (Sect. 2)\n",
    "* Semantic segmentation (Sect. 3)\n",
    "* Adversarial attacks (Sect. 4)\n",
    "\n",
    "In the first part, you will train an end-to-end neural network for image classification. In the second part, you will do the same for semantic segmentation. For these two tasks we expect you to put a significant effort into optimizing performance and as such competing with fellow students via the Kaggle competition. In the third part, you will try to find and exploit the weaknesses of your classification and/or segmentation network. For the latter there is no competition format, but we do expect you to put significant effort in achieving good performance on the self-posed goal for that part. Finally, we ask you to reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision (Sect. 5). It is important to note that only a small part of the grade will reflect the actual performance of your networks. However, we do expect all things to work! In general, we will evaluate the correctness of your approach and your understanding of what you have done that you demonstrate in the descriptions and discussions in the final notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Deep learning resources\n",
    "If you did not yet explore this in *Group assignment 1 (Sect. 2)*, we recommend using the TensorFlow and/or Keras library for building deep learning models. You can find a nice crash course [here](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import resize\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision.transforms import functional as TF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import random\n",
    "from torchvision.transforms import InterpolationMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 PASCAL VOC 2009\n",
    "For this project you will be using the [PASCAL VOC 2009](http://host.robots.ox.ac.uk/pascal/VOC/voc2009/index.html) dataset. This dataset consists of colour images of various scenes with different object classes (e.g. animal: *bird, cat, ...*; vehicle: *aeroplane, bicycle, ...*), totalling 20 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training data\n",
    "train_df = pd.read_csv('/kaggle/input/kul-computer-vision-ga-2-2025/train/train_set.csv', index_col=\"Id\")\n",
    "# train_df =pd.read_csv('train/train_set.csv', index_col=\"Id\")\n",
    "labels = train_df.columns\n",
    "train_df[\"img\"] = [np.load('/kaggle/input/kul-computer-vision-ga-2-2025/train/img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "train_df[\"seg\"] = [np.load('/kaggle/input/kul-computer-vision-ga-2-2025/train/seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "# train_df[\"img\"] = [np.load('train/img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "# train_df[\"seg\"] = [np.load('train/seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "print(\"The training set contains {} examples.\".format(len(train_df)))\n",
    "\n",
    "# Show some examples\n",
    "fig, axs = plt.subplots(2, 20, figsize=(10 * 20, 10 * 2))\n",
    "for i, label in enumerate(labels):\n",
    "    df = train_df.loc[train_df[label] == 1]\n",
    "    axs[0, i].imshow(df.iloc[0][\"img\"], vmin=0, vmax=255)\n",
    "    axs[0, i].set_title(\"\\n\".join(label for label in labels if df.iloc[0][label] == 1), fontsize=40)\n",
    "    axs[0, i].axis(\"off\")\n",
    "    axs[1, i].imshow(df.iloc[0][\"seg\"], vmin=0, vmax=20)  # with the absolute color scale it will be clear that the arrays in the \"seg\" column are label maps (labels in [0, 20])\n",
    "    axs[1, i].axis(\"off\")\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# The training dataframe contains for each image 20 columns with the ground truth classification labels and 20 column with the ground truth segmentation maps for each class\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the test data\n",
    "test_df = pd.read_csv('/kaggle/input/kul-computer-vision-ga-2-2025/test/test_set.csv', index_col=\"Id\")\n",
    "test_df[\"img\"] = [np.load('/kaggle/input/kul-computer-vision-ga-2-2025/test/img/test_{}.npy'.format(idx)) for idx, _ in test_df.iterrows()]\n",
    "# test_df = pd.read_csv('test/test_set.csv', index_col=\"Id\")\n",
    "# test_df[\"img\"] = [np.load('test/img/test_{}.npy'.format(idx)) for idx, _ in test_df.iterrows()]\n",
    "test_df[\"seg\"] = [-1 * np.ones(img.shape[:2], dtype=np.int8) for img in test_df[\"img\"]]\n",
    "print(\"The test set contains {} examples.\".format(len(test_df)))\n",
    "\n",
    "# The test dataframe is similar to the training dataframe, but here the values are -1 --> your task is to fill in these as good as possible in Sect. 2 and Sect. 3; in Sect. 6 this dataframe is automatically transformed in the submission CSV!\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Your Kaggle submission\n",
    "Your filled test dataframe (during Sect. 2 and Sect. 3) must be converted to a submission.csv with two rows per example (one for classification and one for segmentation) and with only a single prediction column (the multi-class/label predictions running length encoded). You don't need to edit this section. Just make sure to call this function at the right position in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rle_encode(img):\n",
    "    \"\"\"\n",
    "    Kaggle requires RLE encoded predictions for computation of the Dice score (https://www.kaggle.com/lifa08/run-length-encode-and-decode)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img: np.ndarray - binary img array\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rle: String - running length encoded version of img\n",
    "    \"\"\"\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    rle = ' '.join(str(x) for x in runs)\n",
    "    return rle\n",
    "\n",
    "def generate_submission(df):\n",
    "    \"\"\"\n",
    "    Make sure to call this function once after you completed Sect. 2 and Sect. 3! It transforms and writes your test dataframe into a submission.csv file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame - filled dataframe that needs to be converted\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    submission_df: pd.DataFrame - df in submission format.\n",
    "    \"\"\"\n",
    "    df_dict = {\"Id\": [], \"Predicted\": []}\n",
    "    for idx, _ in df.iterrows():\n",
    "        df_dict[\"Id\"].append(f\"{idx}_classification\")\n",
    "        df_dict[\"Predicted\"].append(_rle_encode(np.array(df.loc[idx, labels])))\n",
    "        df_dict[\"Id\"].append(f\"{idx}_segmentation\")\n",
    "        df_dict[\"Predicted\"].append(_rle_encode(np.array([df.loc[idx, \"seg\"] == j + 1 for j in range(len(labels))])))\n",
    "    \n",
    "    submission_df = pd.DataFrame(data=df_dict, dtype=str).set_index(\"Id\")\n",
    "    submission_df.to_csv(\"submission.csv\")\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Image classification\n",
    "The goal here is simple: implement a classification CNN and train it to recognise all 20 classes (and/or background) using the training set and compete on the test set (by filling in the classification columns in the test dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomClassificationModel:\n",
    "    \"\"\"\n",
    "    Random classification model: \n",
    "        - generates random labels for the inputs based on the class distribution observed during training\n",
    "        - assumes an input can have multiple labels\n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Adjusts the class ratio variable to the one observed in y. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "        y: list of arrays - n x (nb_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.distribution = np.mean(y, axis=0)\n",
    "        print(\"Setting class distribution to:\\n{}\".format(\"\\n\".join(f\"{label}: {p}\" for label, p in zip(labels, self.distribution))))\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts for each input a label.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: list of arrays - n x (nb_classes)\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "        return [np.array([int(np.random.rand() < p) for p in self.distribution]) for _ in X]\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "model = RandomClassificationModel()\n",
    "model.fit(train_df[\"img\"], train_df[labels])\n",
    "test_df.loc[:, labels] = model.predict(test_df[\"img\"])\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Semantic segmentation\n",
    "The goal here is to implement a segmentation CNN that labels every pixel in the image as belonging to one of the 20 classes (and/or background). Use the training set to train your CNN and compete on the test set (by filling in the segmentation column in the test dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSegmentationModel:\n",
    "    \"\"\"\n",
    "    Random segmentation model: \n",
    "        - generates random label maps for the inputs based on the class distributions observed during training\n",
    "        - every pixel in an input can only have one label\n",
    "    \"\"\"\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Adjusts the class ratio variable to the one observed in Y. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "        Y: list of arrays - n x (height x width)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.distribution = np.mean([[np.sum(Y_ == i) / Y_.size for i in range(len(labels) + 1)] for Y_ in Y], axis=0)\n",
    "        print(\"Setting class distribution to:\\nbackground: {}\\n{}\".format(self.distribution[0], \"\\n\".join(f\"{label}: {p}\" for label, p in zip(labels, self.distribution[1:]))))\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts for each input a label map.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Y_pred: list of arrays - n x (height x width)\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "        return [np.random.choice(np.arange(len(labels) + 1), size=X_.shape[:2], p=self.distribution) for X_ in X]\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "model = RandomSegmentationModel()\n",
    "model.fit(train_df[\"img\"], train_df[\"seg\"])\n",
    "test_df.loc[:, \"seg\"] = model.predict(test_df[\"img\"])\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic segmentation from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic segmentation involves classifying each pixel in an image into one of several predefined categories. It provides a dense, pixel-level understanding of the visual scene. \n",
    "\n",
    "Implemented U-Net uses skip connections, so features from the contracting path are concatenated with those from expanding path. Also it is a symmetrical architecture featuring encoder-decoder structure with matching levels so all downsampling steps has a corresponding unsampling step. Additional bottleneck layer captures higher-level features before upsampling. It can provide better feature presentation because double convolutional block at each level helps learning more robust features.\n",
    "\n",
    "The connection of traditional convolutional neural network with skip connections was used to help with loosing resolution with feature extraction. Encoder has four layers were each of them has double convolutional layer 3x3 with batch normalisation and ReLu activation. ReLU is nonlinear activation function to help to learn more advanced fearures. Batch normalisation stabilises training by normalising activations towards to reach mean value close to 0 and variance close to 1. First layer of encoder has 64 filters converting input image 126x128x3 every next doubles number of filters and at the same time it shrinks the spacial resolution. Bottelneck layer is the deeperst part of network and operates on the most abstract data. It  has the resolution of 8x8x512 and the double convolution 3x3 was used to get 1024 channels. Decoder does the same thing as encoder but instead of max pooling it uses transposed convolution for upsampling. Each block starts with upsampling then concatenates the corresponding encoder features and finally applies two 3x3 convolutions. The number of filters shrinks with the growing resolution. Skip connection halps with connection of encoder features with decoder so connect information about context with localisation.\n",
    "Output layer is 1x1 and transforms 64-channel feature map into the required number of selected classes and generates probability map of each class in each pixel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "#Number of classes for segmentation including background class\n",
    "NUM_CLASSES = 21 #20 classes + background\n",
    "#target size (height, width) tp which all input images and masks will be resized - for the size consistancy in the network\n",
    "TARGET_SIZE = (128, 128)\n",
    "#Dataset\n",
    "#Define dataset for semantic segmentation - loading and preprocessin of image and mask pairs\n",
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Constructor for dataset \n",
    "    Initializes the dataset with a dataframe containing file paths or image/mask data,\n",
    "    also sets the target size for resizing and defines image transformations\n",
    "    \"\"\"\n",
    "    def __init__(self, df, target_size=(128, 128), augment=False):\n",
    "        #store the image data from dataframe\n",
    "        self.images = df[\"img\"].values\n",
    "        #store the mask data from dataframe\n",
    "        self.masks = df[\"seg\"].values\n",
    "        #store the target size for resizing\n",
    "        self.target_size = target_size\n",
    "        self.augment = augment\n",
    "        #Define image transformations: ToTensor converts the image to PyTorch Tensor\n",
    "        # It also scales pixel values from [0,255] to [0,1].\n",
    "        # Normalize the tensor with given mean and standard deviation which helps standarizing the input data distribution \n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "    #return the total number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    #retrive the single sample at the given index\n",
    "    def __getitem__(self, idx):\n",
    "        #Load the image and mask data at the specified index and convert it to uint8 data type\n",
    "        img = Image.fromarray(self.images[idx].astype(np.uint8))\n",
    "        mask = Image.fromarray(self.masks[idx].astype(np.uint8))\n",
    "\n",
    "        if self.augment: \n",
    "            if random.random() > .5:\n",
    "                img = TF.hflip(img)\n",
    "                mask = TF.hflip(mask)\n",
    "            angle = random.choice([0, 90, 180, 270])\n",
    "            #Resize image to the target size\n",
    "            img = TF.rotate(img, angle, interpolation = InterpolationMode.BILINEAR)\n",
    "            #Resize the mask tensor to the target size\n",
    "            #Using InterpolationMode.NEAREST is crucial for masks to preserve discrete class labels\n",
    "            mask = TF.rotate(mask, angle, interpolation=InterpolationMode.NEAREST)\n",
    "        img = TF.resize(img, self.target_size, interpolation=InterpolationMode.BILINEAR)\n",
    "        mask = TF.resize(mask, self.target_size, interpolation=InterpolationMode.NEAREST)\n",
    "        #Apply the defined image transformations to the image\n",
    "        img = self.img_transform(img)\n",
    "        #Convert the mask numpy array to a PyTorch Tensor amd add a channel dimension using unsqueeze\n",
    "        #Convert to float initially, as resize expects float tensors\n",
    "        mask = torch.as_tensor(np.array(mask), dtype=torch.long)\n",
    "\n",
    "        #Return the processed image and mask tensors\n",
    "        return img, mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, val_split=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split a dataframe into training and validation sets for evaluation\n",
    "    of the model performance on unseen data during training process\n",
    "\n",
    "    Use train_test_split from scikit-learn to perform the split.\n",
    "    df => The input dataframe\n",
    "    test_size => The proportion of the dataset to include in the validation split\n",
    "    random_state => Ensure reproducibility of the split\n",
    "    shuffle => Shuffle the data before splitting, important for preventing ordered biases\n",
    "    \"\"\"\n",
    "    train_df, val_df = train_test_split(df,test_size=val_split,\n",
    "        random_state=random_state, shuffle=True)\n",
    "    return train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
    "\n",
    "#Split the main training dataframe into training and validation sets which provides data for training and evaluating the model during the training process\n",
    "train_df, val_df = split_dataframe(train_df)\n",
    "\n",
    "\"\"\"\n",
    "    Create dataset and dataloader for training and validation data\n",
    "    DataLoader provides an iterable over the dataset, handling batching, shuffling, and multiprocessing\n",
    "    batch_size => The number of samples per batch.\n",
    "    shuffle => Shuffle the data at each epoch\n",
    "    num_workers => Number of subprocesses to use for data loading(0 means the main process)\n",
    "\"\"\"\n",
    "train_dataset = SegmentationDataset(train_df, target_size=TARGET_SIZE)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = SegmentationDataset(val_df, target_size=TARGET_SIZE)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision about this specific network was based on fact that the U-Net networks gives a bit better representation of shape of the structures. Approach with classic convolutional neural network step by step shrinks the resolution of the representation in each layer which results in getting small feature maps which are good for classifications of whole image. Getting the full segmentation mask from that may affect in not emphasising the curves and the borders of objects may be lost. U-Net with its more complexed architecture that includes encoder and decoder solves this problem a bit. Encoder accts like a bit like classical version of CNN decreasing the resolution and enhances number of channels to get deep semantic context. But on the other hand decoder with each step uses transposition and in implemented case also skip-connection where the feature map from encoder is added to the representation. The network gets  new information in every reconstruction layer about textures or corves which helps in emphasis of contours of the objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\" \n",
    "        Define of UNet model\n",
    "        It consists of a contracting path (encoder) to capture context and an expanding path (decoder)\n",
    "        to enable precise localization, with skip connections between the encoder and decoder\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        \"\"\"\n",
    "            Args: num_classes(init): number of ourput classes with background\n",
    "        \"\"\"\n",
    "        super().__init__() #construct the parent class nn.Module\n",
    "        self.num_classes = num_classes #store the number of classes\n",
    "        \n",
    "        \"\"\"\n",
    "            Contracting Path - Encoder\n",
    "            The network downsamples the input image and extracts features. \n",
    "            Each downsampling block consists of convolutional layers and a pooling layer and\n",
    "            the number of channels increases with depth to capture more complex features\n",
    "        \"\"\"\n",
    "        # First double convolution block: Input channels = 3 (for RGB images), Output channels = 64\n",
    "        self.down_conv1 = self.double_conv(3, 64)\n",
    "        # Second double convolution block: Input channels = 64, Output channels = 128\n",
    "        self.down_conv2 = self.double_conv(64, 128)\n",
    "        # Third double convolution block: Input channels = 128, Output channels = 256\n",
    "        self.down_conv3 = self.double_conv(128, 256)\n",
    "        # Fourth double convolution block: Input channels = 256, Output channels = 512\n",
    "        self.down_conv4 = self.double_conv(256, 512)\n",
    "        # Max pooling layer for downsampling => kernel size and stride of 2 reduce the spatial dimensions by half\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck - he layer with the lowest spatial resolution and highest number of channels connecting encoder and decoder\n",
    "        self.bottleneck = self.double_conv(512, 1024)\n",
    "        \n",
    "        \"\"\"\n",
    "            Expanding Path - Decoder\n",
    "            The network upsamples here the feature maps and reconstructs the segmentation mask.\n",
    "            It uses transposed convolutions (or upsampling followed by convolution) and skip connections.\n",
    "            The number of channels decreases with depth.\n",
    "        \"\"\"\n",
    "        # First transposed convolution for upsampling from the bottleneck\n",
    "        # Input channels = 1024, Output channels = 512. Kernel size and stride of 2 double the spatial dimensions.\n",
    "        self.up_trans1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        # First up-convolution block after the skip connection\n",
    "        # Input channels = 1024 (512 from transposed conv + 512 from skip connection), Output channels = 512\n",
    "        self.up_conv1 = self.double_conv(1024, 512)\n",
    "        # Second transposed convolution for upsampling\n",
    "        # Input channels = 512, Output channels = 256\n",
    "\n",
    "        self.up_trans2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        # Second up-convolution block after the skip connection\n",
    "        # Input channels = 512 (256 from transposed conv + 256 from skip connection), Output channels = 256\n",
    "        self.up_conv2 = self.double_conv(512, 256)\n",
    "\n",
    "        # Third transposed convolution for upsampling\n",
    "        # Input channels = 256, Output channels = 128\n",
    "        self.up_trans3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        # Third up-convolution block after the skip connection\n",
    "        # Input channels = 256 (128 from transposed conv + 128 from skip connection), Output channels = 128\n",
    "        self.up_conv3 = self.double_conv(256, 128)\n",
    "\n",
    "        # Fourth transposed convolution for upsampling\n",
    "        # Input channels = 128, Output channels = 64\n",
    "        self.up_trans4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        # Fourth up-convolution block after the skip connection\n",
    "        # Input channels = 128 (64 from transposed conv + 64 from skip connection), Output channels = 64\n",
    "        self.up_conv4 = self.double_conv(128, 64)\n",
    "        \n",
    "        # Final output layer\n",
    "        # A 1x1 convolution to map the final feature maps to the number of classes\n",
    "        # Input channels = 64, Output channels = num_classes\n",
    "        self.out_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "    \n",
    "    #Double convolutional block function  Consists of two convolutional layers, each followed by batch normalization and ReLU activation\n",
    "    def double_conv(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "            Double convolution block: Conv -> BatchNorm -> ReLU -> Conv -> BatchNorm -> ReLU\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            # First convolutional layer. Kernel size 3x3, padding 1 to maintain spatial dimensions\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            # Batch normalization layer to normalize the activations, improving training stability\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            # ReLU activation function for non-linearity. inplace=True saves memory\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Second convolutional layer\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            # Second batch normalization layer\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            # Second ReLU activation function\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Defines the forward pass of the U-Net model\n",
    "            Args:\n",
    "                x: The input tensor (image batch)\n",
    "        \"\"\"\n",
    "        # Forward pass through Encoder\n",
    "        # Apply the first double convolution block. Store the output (x1) for the skip connection\n",
    "        x1 = self.down_conv1(x)\n",
    "        # Apply max pooling to reduce spatial dimensions\n",
    "        x2 = self.maxpool(x1)\n",
    "        \n",
    "        # Apply the second double convolution block. Store the output (x3) for the skip connection\n",
    "        x3 = self.down_conv2(x2) \n",
    "        # Apply max pooling\n",
    "        x4 = self.maxpool(x3)\n",
    "        \n",
    "        # Apply the third double convolution block. Store the output (x5) for the skip connection\n",
    "        x5 = self.down_conv3(x4)\n",
    "        # Apply max pooling\n",
    "        x6 = self.maxpool(x5)\n",
    "        \n",
    "        # Apply the fourth double convolution block. Store the output (x7) for the skip connection\n",
    "        x7 = self.down_conv4(x6)\n",
    "        # Apply max pooling => the input to the bottleneck\n",
    "        x8 = self.maxpool(x7)\n",
    "        \n",
    "        # Bottleneck - Apply the bottleneck double convolution block\n",
    "        x9 = self.bottleneck(x8)\n",
    "        \n",
    "        # Forward pass through the Decoder \n",
    "        # Apply the first transposed convolution to upsample from the bottleneck\n",
    "        x = self.up_trans1(x9)\n",
    "        # Concatenate the upsampled feature map with the corresponding feature map from the encoder (x7)\n",
    "        # This is the skip connection, providing high-resolution features to the decoder.\n",
    "        # dim=1 means concatenating along the channel dimension\n",
    "        x = torch.cat([x, x7], dim=1)  # Skip connection\n",
    "        # Apply the first up-convolution block\n",
    "        x = self.up_conv1(x)\n",
    "        \n",
    "        # Apply the second transposed convolution\n",
    "        x = self.up_trans2(x)\n",
    "        # Concatenate with the feature map from the encoder (x5)\n",
    "        x = torch.cat([x, x5], dim=1)  # Skip connection\n",
    "        # Apply the second up-convolution block\n",
    "        x = self.up_conv2(x)\n",
    "        \n",
    "        # Apply the third transposed convolution\n",
    "        x = self.up_trans3(x)\n",
    "        # Concatenate with the feature map from the encoder (x3)\n",
    "        x = torch.cat([x, x3], dim=1)  # Skip connection\n",
    "        # Apply the third up-convolution block\n",
    "        x = self.up_conv3(x)\n",
    "        \n",
    "        # Apply the fourth transposed convolution\n",
    "        x = self.up_trans4(x)\n",
    "        # Concatenate with the feature map from the encoder (x1)\n",
    "        x = torch.cat([x, x1], dim=1)  # Skip connection\n",
    "        # Apply the fourth up-convolution block\n",
    "        x = self.up_conv4(x)\n",
    "        \n",
    "        # Final output => Apply the 1x1 convolution to produce the final segmentation map\n",
    "        out = self.out_conv(x)\n",
    "        #Return the output tensor \n",
    "        return out\n",
    "\n",
    "#Set device for training (GPU if available otherwise use CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Initialize U-Net model and move it to the selected device\n",
    "model = UNet(NUM_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing combination of two training losses emphasises positive aspects of both of them. Mainly the Cross-Entropy focuses on measuring the pixels separetly and at the same time in Dice the overlap of whole spaces. The first one may lose some information about some small object and may not emphasise them properly and the second one adds extra pressure for smaller elements. So combinantion allows for Dice to be aware of small object and Cross-Entropy holds all pixels. With only using Cross-Entropy the segmentation was even more disrupted/blurry but this method converges faster then Dice which allows to mark contours a bit more percise. The used technique of combined losses combines the main features of the methods and allows to better training for notbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Used as loss function especially when there is class imbalance\n",
    "    It measures the similarity between the predicted segmentation and the ground truth mask\n",
    "    Args:\n",
    "        smooth => A small value added to the numerator and denominator to prevent division by zero\n",
    "        ignore_index => Class index to ignore in the loss calculation for example invalid regions\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1, ignore_index=255):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "        self.smooth = smooth # Store the smoothing value\n",
    "        self.ignore_index = ignore_index # Store the index to ignore\n",
    "\n",
    "    \"\"\"\n",
    "        Forward pass for the Dice Loss calculation\n",
    "        Args:\n",
    "            pred => The predicted segmentation map\n",
    "            target=> The ground truth segmentation mask\n",
    "    \"\"\"\n",
    "    def forward(self, pred, target):\n",
    "        # Apply softmax to the predicted logits to get probabilities for each class\n",
    "        # dim=1 means applying softmax across the channel dimension\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        # Get the number of classes from the prediction tensor\n",
    "        num_classes = pred.shape[1]\n",
    "        \n",
    "        # Create a mask to exclude pixels with the ignore_index from the loss calculation\n",
    "        mask = (target != self.ignore_index).float()\n",
    "        #Apply the mask to the target, convert back to long as target should have class indices\n",
    "        target = target * mask.long()\n",
    "        \n",
    "        # Convert the target mask to one-hot encoding\n",
    "        #Create a binary tensor where for each pixel, only the channel corresponding to the\n",
    "        # ground truth class is 1, and others are 0\n",
    "        target_onehot = torch.nn.functional.one_hot(target, num_classes=num_classes).permute(0,3,1,2)\n",
    "        \n",
    "        # Calculate the intersection between the predicted probabilities and the one-hot target\n",
    "        # Sum across the spatial dimensions (height and width) to get the intersection for each class in each batch\n",
    "        intersection = (pred * target_onehot).sum(dim=(2,3))\n",
    "        # Calculate the union of the predicted probabilities and the one-hot target and sum across the spatial dimensions\n",
    "        union = pred.sum(dim=(2,3)) + target_onehot.sum(dim=(2,3))\n",
    "        # Calculate the Dice coefficient for each class in each batch\n",
    "        # Add 'smooth' to numerator and denominator to avoid division by zero\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "\n",
    "        # Return the mean Dice loss (1 - Dice coefficient) averaged across all classes and batches.\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "\"\"\"\"\n",
    "Define a combined loss function that is a weighted sum of Cross-Entropy Loss and Dice Loss.\n",
    "Using a combination of loss functions can often lead to better performance, especially\n",
    "for segmentation tasks with class imbalance. Cross-Entropy focuses on individual pixel classification,\n",
    "while Dice Loss focuses on the overall overlap of segmentation regions\n",
    "\"\"\"\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        weight => Class weights for Cross-Entropy Loss to handle class imbalance\n",
    "        alpha => Weighting factor for the Cross-Entropy Loss (1 - alpha is the weight for Dice Loss)\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        # Initialize the Cross-Entropy Loss\n",
    "        self.ce_loss = nn.CrossEntropyLoss(weight=weight)\n",
    "        # Initialize the Dice Loss\n",
    "        self.dice_loss = DiceLoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Define the forward pass for the Combined Loss calculation\n",
    "        pred => The predicted segmentation map \n",
    "        target => The ground truth segmentation mask\n",
    "        \"\"\"\n",
    "        # Calculate the Cross-Entropy Loss\n",
    "        ce = self.ce_loss(pred, target)\n",
    "        # Calculate the Dice Loss\n",
    "        dice = self.dice_loss(pred, target)\n",
    "        # Return the weighted sum of the two losses\n",
    "        return self.alpha * ce + (1 - self.alpha) * dice\n",
    "    \n",
    "\"\"\"\n",
    "    Class weighting for impalanced datasets\n",
    "    Class weighting assigns higher importance to less frequent classes during training,\n",
    "    helping the model learn to segment them better\n",
    "\"\"\"\n",
    "# Calculate the count of pixels for each class in the training masks\n",
    "# np.concatenate joins all mask arrays into a single array,\n",
    "# np.bincount counts the occurrences of each non-negative integer value\n",
    "class_counts = np.bincount(np.concatenate([m.flatten() for m in train_df[\"seg\"]]))\n",
    "# Calculate initial class weights as the inverse of class counts\n",
    "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float32)\n",
    "# Normalize the class weights so they sum to 1, ensurING that the overall scale of the weighted loss is consistent\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "# Initialize the combined loss function with calculated class weights and an alpha value\n",
    "criterion = CombinedLoss(weight=class_weights.to(device), alpha=0.5)\n",
    "\n",
    "\"\"\"\n",
    "    Adam optimization algorithm  adapts the learning rate for each parameter\n",
    "    Args:\n",
    "        model.parameters()=> specifies which parameters of the model should be optimized\n",
    "        lr => learning rate, controls the step size during optimization\n",
    "        weight_decay => L2 regularization term, helps prevent overfitting by penalizing large weights\n",
    "\"\"\"\n",
    "#Selection optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "# Choose a learning rate scheduler to adjust the learning rate during training\n",
    "# 'min' =>  Monitor a metric that should be minimized (validation loss)\n",
    "# patience => Number of epochs with no improvement after which the learning rate will be reduced\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizualise train losses\n",
    "# Lists to store training and validation losses for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Number of epochs to train the model => An epoch is one full pass through the training dataset\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    # Initialize running loss for the current epoch\n",
    "    running_loss = 0.0\n",
    "    # Iterate over batches in the training data loader\n",
    "    for imgs, masks in train_loader:\n",
    "        # Move images and masks to the selected device\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        \n",
    "        # Zero the gradients of the model parameters\n",
    "        # Gradients are accumulated by default, so this is necessary to prevent\n",
    "        # gradients from previous iterations affecting the current update\n",
    "        optimizer.zero_grad()\n",
    "        # Perform a forward pass to get model predictions for the current batch of images\n",
    "        outputs = model(imgs) \n",
    "        # Calculate the loss using the defined criterion and the predictions and ground truth masks\n",
    "        loss = criterion(outputs, masks)\n",
    "        # Perform backpropagation - calculate gradients of the loss with respect to the model parameters\n",
    "        loss.backward()\n",
    "        # Update the model parameters using the calculated gradients and the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss for the current epoch\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss/len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    # Set the model to evaluation mode.\n",
    "    model.eval()\n",
    "    # Initialize validation loss for the current epoch\n",
    "    val_loss = 0.0\n",
    "    # Disable gradient calculation during validation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches in the validation data loader\n",
    "        for imgs, masks in val_loader:\n",
    "            # Move images and masks to the selected device\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            # Perform a forward pass to get model predictions\n",
    "            outputs = model(imgs)\n",
    "            # Calculate the loss on the validation data\n",
    "            loss = criterion(outputs, masks)\n",
    "            # Accumulate the validation loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Calculate the average validation loss for the epoch\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot the training loss over epochs\n",
    "plt.plot(range(1, num_epochs+1), train_losses, 'b-o', label='Training Loss', linewidth=2, markersize=8)\n",
    "# Plot the validation loss over epochs\n",
    "plt.plot(range(1, num_epochs+1), val_losses, 'r-o', label='Validation Loss',linewidth=2, markersize=8)\n",
    "plt.title('Training and Validation Loss over Epochs', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.xticks(range(1, num_epochs+1))\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the test results\n",
    "def show_predictions(model, dataloader, num_show):\n",
    "    # Set the model to evaluation mode.\n",
    "    model.eval()\n",
    "    imgs, masks = next(iter(dataloader))\n",
    "    imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "    # Disable gradient calculation for predictions\n",
    "    with torch.no_grad():\n",
    "        preds = model(imgs)\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "    #Convert to numpy for visualization\n",
    "    imgs_np = imgs.cpu().numpy()\n",
    "    masks_np = masks.cpu().numpy()\n",
    "    preds_np = preds.cpu().numpy()\n",
    "\n",
    "    # Reverse normalization applied during preprocessing to display the images correctly\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    imgs_np = imgs_np.transpose(0, 2, 3, 1)\n",
    "    imgs_np = imgs_np * std + mean\n",
    "    imgs_np = np.clip(imgs_np, 0, 1)\n",
    "\n",
    "    #Plot results\n",
    "    num_show = min(3, len(imgs))\n",
    "    _, axs = plt.subplots(num_show, 3, figsize=(15, 5*num_show))\n",
    "\n",
    "    # Iterate through the selected number of samples\n",
    "    for i in range(num_show):\n",
    "        axs[i, 0].imshow(imgs_np[i])\n",
    "        axs[i, 0].set_title(\"Input Image\")\n",
    "        axs[i, 0].axis('off')\n",
    "        \n",
    "        axs[i, 1].imshow(masks_np[i], vmin=0, vmax=NUM_CLASSES-1, cmap='jet')\n",
    "        axs[i, 1].set_title(\"Ground Truth\")\n",
    "        axs[i, 1].axis('off')\n",
    "        \n",
    "        axs[i, 2].imshow(preds_np[i], vmin=0, vmax=NUM_CLASSES-1, cmap='jet')\n",
    "        axs[i, 2].set_title(\"Prediction\")\n",
    "        axs[i, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_predictions(model, val_loader, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install crfseg\n",
    "!pip install git+https://github.com/lucasb-eyer/pydensecrf.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models.segmentation as models\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "import matplotlib.colors as mcolors\n",
    "import pydensecrf.densecrf as dcrf\n",
    "from pydensecrf.utils import unary_from_softmax, create_pairwise_bilateral\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "import cv2\n",
    "from tqdm import trange, tqdm\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 0\n",
    "EPOCH = 70\n",
    "N_frozen = 3\n",
    "LR = 1e-5\n",
    "LR_FROZEN = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    elif hasattr(torch, 'xla') and torch.xla.device_count() > 0:\n",
    "        return torch.device('xla')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "A customized dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2009Dataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, target_transform=None, paired_transform=None, ignore_label=21):\n",
    "        self.df = dataframe.reset_index()\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.paired_transform = paired_transform\n",
    "        \n",
    "        self.ignore_label = ignore_label\n",
    "        self.classes = 22  # 20 classes + background + void\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[Tensor, Tensor]:\n",
    "        image = self.df.iloc[idx]['img'] # np.array\n",
    "        mask = self.df.iloc[idx]['seg']   \n",
    "\n",
    "        image = Image.fromarray(image.astype(np.uint8))  \n",
    "        mask = Image.fromarray(mask.astype(np.uint8))    \n",
    "\n",
    "        if self.paired_transform:\n",
    "            image, mask = self.paired_transform(image, mask)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask # (C, H, W), (H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms\n",
    "Firstly, masks and images are together resized to (256, 256), and then augmented via geometric augmentation and photometric augmentation for preventing overfitting. Then images are normalized via ImageNet normalization, since it is a standard procedure for the used models. A paired_transform without augmentation is also defined for valdation dataset and test dataset. During interpolation, image is interpolated using bilinear interpolation, and nearest neighbor is applied to the mask, because image pixel values are continuous and mask pixel values are 21 labels.<br>\n",
    "\n",
    "Augmentation is observed to matigate the overfitting greatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for these transform functions are PIL.Image\n",
    "paired_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(\n",
    "            256, 256,\n",
    "            interpolation=cv2.INTER_LINEAR, # Linear interpolation for continuous values\n",
    "            mask_interpolation=cv2.INTER_NEAREST, # Nearest neighbours for discrete (label) values\n",
    "        ),\n",
    "    ],\n",
    "    additional_targets={'mask': 'mask'}\n",
    ")\n",
    "\n",
    "paired_transform_aug = A.Compose(\n",
    "    [\n",
    "        A.Resize(\n",
    "            256, 256,\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            mask_interpolation=cv2.INTER_NEAREST,\n",
    "        ),\n",
    "\n",
    "        # geometric flips & rotations\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "\n",
    "        # small random affine (shift/scale/rotate)\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.0625,  # up to 6.25% shift\n",
    "            scale_limit=0.1,     # up to 10% zoom\n",
    "            rotate_limit=15,     # up to 15\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            mask_interpolation=cv2.INTER_NEAREST,\n",
    "            p=0.5\n",
    "        ),\n",
    "\n",
    "        # elastic / grid warps for shape variation\n",
    "        A.ElasticTransform(\n",
    "            alpha=1, sigma=50, alpha_affine=50,\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            mask_interpolation=cv2.INTER_NEAREST,\n",
    "            p=0.2\n",
    "        ),\n",
    "        A.GridDistortion(\n",
    "            distort_limit=0.3,\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            mask_interpolation=cv2.INTER_NEAREST,\n",
    "            p=0.2\n",
    "        ),\n",
    "\n",
    "        # photometric changes (image-only)\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "    ],\n",
    "    additional_targets={'mask': 'mask'}\n",
    ")\n",
    "\n",
    "# Inputs PIL image, outputs tensor (C, H, W)\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Changes to channel first, (C, H, W)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225]), # Normalize for three channels\n",
    "])\n",
    "\n",
    "# Inputs PIL image, outputs tensor (H, W)\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.tensor(np.array(x), dtype=torch.long)),\n",
    "    transforms.Lambda(lambda x: torch.where(x == 255, 21, x)), # Map void labels to 21 for one-hot encoding later\n",
    "])\n",
    "\n",
    "def apply_paired_transform(image: Image.Image, mask: Image.Image) -> Tuple[Image.Image, Image.Image]:\n",
    "    image_np = np.array(image)\n",
    "    mask_np  = np.array(mask)\n",
    "    resized = paired_transform(image=image_np, mask=mask_np)\n",
    "    image_resized = Image.fromarray(resized['image'])\n",
    "    mask_resized  = Image.fromarray(resized['mask'])\n",
    "    \n",
    "    return image_resized, mask_resized # (H, W, 3), (H, W)\n",
    "\n",
    "def apply_paired_transform_aug(image: Image.Image, mask: Image.Image) -> Tuple[Image.Image, Image.Image]:\n",
    "    image_np = np.array(image)\n",
    "    mask_np  = np.array(mask)\n",
    "    resized = paired_transform_aug(image=image_np, mask=mask_np)\n",
    "    image_resized = Image.fromarray(resized['image'])\n",
    "    mask_resized  = Image.fromarray(resized['mask'])\n",
    "    \n",
    "    return image_resized, mask_resized # (H, W, 3), (H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train and val dataset for training and validation.<br>\n",
    "No augmentation is used for valdation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df: pd.DataFrame, val_split=0.2, random_state=42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    train_df, val_df = train_test_split(\n",
    "        df,\n",
    "        test_size=val_split,\n",
    "        random_state=random_state,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "train_df, val_df = split_dataframe(train_df)\n",
    "\n",
    "train_dataset = VOC2009Dataset(\n",
    "    dataframe=train_df,\n",
    "    transform=image_transform,\n",
    "    target_transform=mask_transform,\n",
    "    paired_transform=apply_paired_transform_aug\n",
    "    )\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "val_dataset = VOC2009Dataset(\n",
    "    dataframe=val_df,\n",
    "    transform=image_transform,\n",
    "    target_transform=mask_transform,\n",
    "    paired_transform=apply_paired_transform\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "A customized EarlyStopping wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=1e-4, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.delta = delta # Minimum improvement\n",
    "        self.verbose = verbose\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss  # Convert to negative if minimizing loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.4f} --> {val_loss:.4f}). Saving model...')\n",
    "        torch.save(model.state_dict(), 'segmentation_transferlearning_checkpoint.pt')\n",
    "        self.best_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DiceLoss\n",
    "A customized DiceLoss for transfer learning for segmentation.<br>\n",
    "During calculating the loss, all values whose labels == 21 (which means void, mapped from 255 during the preprocessing) are ignored. This is done by masking all void values as 0 during one-hot encoding using a numpy mask. This is more logical compared with simply mapping void pixels to background pixels or others. In the implementation, both the average DICE loss and the DICE score per class are returned.\n",
    "\n",
    "Explanation of DICE:<br>\n",
    "It is quite similar to IoU. The DICE coeffcient is calculated as the amount of matching pixels divided by the total number of pixels of both the image and the mask. There is also a smooth item at both the numerator and denominator. And DICE loss = 1 - DICE coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1, ignore_index=21):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, pred, target) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # pred: [B, C, H, W], probabilities, not logits\n",
    "        # target: [B, H, W]\n",
    "        num_classes = pred.size(1) + 1\n",
    "        mask = (target != self.ignore_index).float() \n",
    "        mask = mask.unsqueeze(1) # [B, 1, H, W]\n",
    "\n",
    "        target = torch.nn.functional.one_hot(target.long(), num_classes=num_classes)  # [batch_size, 1, height, width, num_classes]\n",
    "        target = target.squeeze(1).permute(0, 3, 1, 2) # [batch_size, num_classes, height, width]]\n",
    "        \n",
    "        # Apply mask to target\n",
    "        mask_target = mask.expand_as(target)  # [batch_size, 22, height, width]\n",
    "        target = target * mask_target  # Zero out ignored pixels\n",
    "        target = target[:, :-1] # [batch_size, 21, height, width]\n",
    "        \n",
    "        # Apply mask to predictions\n",
    "        mask_pred = mask.expand_as(pred) # [batch_size, 21, height, width]\n",
    "        pred = pred * mask_pred \n",
    "\n",
    "        # Flatten predictions and targets for each class\n",
    "        pred = pred.contiguous().view(-1, pred.size(1))  # [batch_size * height * width, num_classes]\n",
    "        target = target.contiguous().view(-1, target.size(1))  # [batch_size * height * width, num_classes]\n",
    "        \n",
    "        # Compute Dice coefficient for each class\n",
    "        intersection = (pred * target).sum(dim=0)  # Sum over pixels for each class\n",
    "        union = pred.sum(dim=0) + target.sum(dim=0)  # Sum over pixels for each class\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth + 1e-8)  # Dice score per class\n",
    "        \n",
    "        # Return 1 - mean Dice score as loss\n",
    "        return 1 - dice.mean(), dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CEDiceLoss\n",
    "A loss combining unweighted cross entropy + Dice loss used for training.\n",
    "There is also a parameter assigning different weights to CE and DICE.\n",
    "\n",
    "Explanation of cross entropy:<br>\n",
    "CE = - sum of log (p_pred * p_truth) <br>\n",
    "So it is maximizing the probability that a pixel is predicted as the truth label, summed over all the pixels.\n",
    "A more advanced CE is weighted CE, which assignes different weights to different classes, where the weights relates to the proportion of the class in the dataset. It can be helpful in dealing with unbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEDiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1, ignore_index=21, alpha=0.5):\n",
    "        super(CEDiceLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.diceloss_fn = DiceLoss(smooth, ignore_index)\n",
    "        self.celoss_fn = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        '''\n",
    "        pred should be logits instead of probabilities here.\n",
    "        pred [B, C, H, W], target [B, H, W]\n",
    "        '''\n",
    "        pred_probs = F.softmax(pred, dim=1)\n",
    "        diceloss, _ = self.diceloss_fn.forward(pred_probs, target)\n",
    "        target_ce = target.squeeze(1)\n",
    "        celoss = self.celoss_fn.forward(pred, target_ce)\n",
    "        return self.alpha * diceloss + (1 - self.alpha) * celoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "It wraps the training and validating process, visulizing loss at the end.<br>\n",
    "The backbone is frozen during several first training epochs as there are already pretrained parameters. <br>\n",
    "Transfer learning without frozen them will mess all them up and making the pretrian less meaningful.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationTrainer:\n",
    "    def __init__(self, model, optimizer_head, scheduler_head, criterion, train_loader, val_loader,\n",
    "                 device, early_stopping=None, freeze_epoch=None, \n",
    "                 optimizer_full=None, scheduler_full=None):\n",
    "\n",
    "        self.model = model.to(device)\n",
    "        self.crit = criterion\n",
    "        self.tr_dl = train_loader\n",
    "        self.val_dl = val_loader\n",
    "        self.device = device\n",
    "\n",
    "        self.optimizer_head = optimizer_head\n",
    "        self.scheduler_head = scheduler_head\n",
    "        self.optimizer_full = optimizer_full\n",
    "        self.scheduler_full = scheduler_full\n",
    "\n",
    "        self.es = early_stopping\n",
    "        self.freeze_epoch = freeze_epoch\n",
    "        self.history = {'train_loss':[], 'val_loss':[]}\n",
    "\n",
    "        self.scheduler = scheduler_head\n",
    "        self.opt = optimizer_head\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for imgs, masks in tqdm(self.tr_dl, desc=f\"Train {epoch}\"): # imgs and masks are tensors\n",
    "            # imgs [B, C, H, W], masks [B, H, W]\n",
    "            imgs, masks = imgs.to(self.device), masks.to(self.device)\n",
    "            logits = self.model(imgs)['out']\n",
    "            loss = self.crit(logits, masks)\n",
    "\n",
    "            self.opt.zero_grad()\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.tr_dl)\n",
    "        self.history['train_loss'].append(avg_loss)\n",
    "        print(f'Training loss: {avg_loss}')\n",
    "        return avg_loss\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in tqdm(self.val_dl, desc=\"Val\"):\n",
    "                imgs, masks = imgs.to(self.device), masks.to(self.device)\n",
    "                logits  = self.model(imgs)['out']\n",
    "                loss = self.crit(logits, masks)\n",
    "                total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(self.val_dl)\n",
    "        self.history['val_loss'].append(avg_loss)\n",
    "        print(f'Val loss: {avg_loss}')\n",
    "        return avg_loss\n",
    "\n",
    "    def unfreeze_check(self, epoch):\n",
    "        if epoch == self.freeze_epoch:\n",
    "            for p in self.model.backbone.parameters():\n",
    "                p.requires_grad = True\n",
    "            # set BatchNorm back to train()\n",
    "            for m in self.model.backbone.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.train()\n",
    "\n",
    "            self.scheduler = self.scheduler_full\n",
    "            self.opt = self.optimizer_full\n",
    "\n",
    "    def fit(self, epochs: int, checkpoint_path='segmentation_transferlearning_checkpoint.pt'):\n",
    "        best_loss = float('inf')\n",
    "        for epoch in range(1, epochs+1):\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            val_loss = self.validate()\n",
    "\n",
    "            # early stopping\n",
    "            if self.es:\n",
    "                self.es(val_loss, self.model)\n",
    "                if self.es.early_stop:\n",
    "                    print(\"Early stopping.\")\n",
    "                    break\n",
    "\n",
    "            # scheduler step\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step(val_loss)\n",
    "\n",
    "            # unfreeze logic\n",
    "            self.unfreeze_check(epoch)\n",
    "\n",
    "        # load best\n",
    "        self.model.load_state_dict(torch.load(checkpoint_path))\n",
    "        return self.history\n",
    "    \n",
    "    def visualize_losses(self, save_path=None):\n",
    "        epochs = list(range(1, len(self.history['train_loss']) + 1))\n",
    "        train_loss = self.history['train_loss']\n",
    "        val_loss = self.history['val_loss']\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, train_loss, label='Training Loss', marker='o')\n",
    "        plt.plot(epochs, val_loss, label='Validation Loss', marker='s')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer and Model Choice\n",
    "The following code uses deeplabv3 using resnet50 as backbone, trained using CE + DICE loss. Various other models with various loss functions are tried but gave smaller scores evaluated using DICE coefficient. The other attempted models with various loss are listed as following:<br>\n",
    "(All weights in CE are carefully tuned) <br>\n",
    "\n",
    "At first DeepLabV3 is chosen as the baseline and to pick the best training loss function. <br>\n",
    "(Model - Loss function - DICE coefficient)<br>\n",
    "DeepLabV3 without transfer learning (Baseline) - 0.900 <br>\n",
    "DeepLabV3 - CE - 0.912 <br>\n",
    "DeepLabV3 - Weighted CE - 0.870 <br>\n",
    "DeepLabV3 - DICE - 0.937<br>\n",
    "DeepLabV3 - CE + DICE - 0.934, slightly worse than using only DICE, but considers more metrics, thus is chosen<br>\n",
    "DeepLabV3 - Weighted CE + DICE - 0.917 <br>\n",
    "Therefore, CE + DICE is chosen as the loss function <br>\n",
    "\n",
    "After selecting the loss function, various models are tried to pick the best one<br>\n",
    "(Model - DICE coefficient)\n",
    "DeepLabV3 + MobileNetV2 - 0.867<br>\n",
    "LinkeNet + ResNet18 - 0.744<br>\n",
    "UNet + ResNet34 - 0.867<br>\n",
    "FPN + ResNet50 - 0.867<br>\n",
    "PSP + ResNet50 - 0.767<br>\n",
    "DeepLabV3 + ResNet50 - 0.927<br>\n",
    "DeepLabV3 + ResNet101 - 0.920<br>\n",
    "DeepLabV3Plus + ResNet50 - 0.800<br>\n",
    "FCN + ResNet50 - 0.915<br>\n",
    "Therefore, DeepLabV3 + Resnet50 is chosen for the task.<br>\n",
    "During this phase, more simpler models compared with DeepLabV3 are tried as there is overfitting observed in DeepLabV3 + ResNet50.\n",
    "\n",
    "Codes for these models are not shown in the notbook as the models are simply called from packages and will take ages to run all of them.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepLabV3 Explanation\n",
    "The biggest feature of DeepLabV3 is atrous convolution, which skips some pixels during convolution so that the reception field is increased without loss much information like pool. Convolutions in the later several layers in ResNet50 are replaced using atrous convulution, and ResNet50 generates a feature map. The feature map will be further processed by a 1\\*1 convolution kernel, 3 atrous convolution kernels with various rates aiming to capturing features at different abstract level, and a image pooling which compresses the whole image information to a single pixel so that the context information is represented, and the single pixel from the image pooling will be expanded to the same size as the three outputs using bilinear interpolation and then all the channels are concatentated. The concatenated channels go through a 1\\*1 convolution head for fusion the information across channels. The whole structure works as an encoder. \n",
    "\n",
    "### DeepLabV3Plus Explanation\n",
    "DeepLabV3Plus is more commonly used than DeepLabV3, which is and encoder-decoder structure with DeepLabV3 as the encoder. It concatenates the output from DeepLabV3 and early features extracted from early conv layers of ResNet50, then fusion then. The result then goes through several small convolutional layers to produce the last result. \n",
    "\n",
    "### Why choose DeepLabV3 not Plus\n",
    "We actually expect DeepLabV3Plus performs better than DeepLabV3 becuase of its light weight decoder. The reason DeepLabV3Plus is worse is that it can ony be imported from segmentation_models_pytorch, which are pretrained on ImageNet dataset. But models imported from torchvision.models.segmentation are trained on COCO. We think COCO represents a closer distribution of the VOC2009 in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.deeplabv3_resnet50(pretrained=True, num_classes=21) \n",
    "criterion = CEDiceLoss(alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is a customized model we also tried. It inserts a CRF layer to DeepLabV3, so its weights can be trained. But the result is slightly worse than the current model (0.927 VS 0.935). It may have introduced unnecessary complexity given DeepLabV3 is already overfitting and CRF layer fails to capture the pixel values distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DeepLabWithCRF(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.deeplab = models.deeplabv3_resnet50(\n",
    "#             pretrained=True,\n",
    "#             num_classes=num_classes\n",
    "#         )\n",
    "#         self.crf = CRF(n_spatial_dims=2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out_dict = self.deeplab(x)\n",
    "#         logits = out_dict['out']           \n",
    "#         refined = self.crf(logits)          # [B, C, H, W], log-probs\n",
    "#         refined = F.softmax(refined, dim=1)\n",
    "#         eps = 1e-6\n",
    "#         logits = torch.log(refined.clamp(min=eps))\n",
    "#         return logits\n",
    "    \n",
    "# model = DeepLabWithCRF(21)\n",
    "# criterion = CEDiceLoss(alpha=0.8)\n",
    "\n",
    "# for param in model.deeplab.backbone.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for m in model.deeplab.backbone.modules():\n",
    "#     if isinstance(m, nn.BatchNorm2d):\n",
    "#         m.eval()\n",
    "\n",
    "# head_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "# optimizer_head = torch.optim.Adam([\n",
    "#     {'params': head_params, 'lr': LR_FROZEN},\n",
    "# ], lr=LR_FROZEN)\n",
    "# scheduler_head = torch.optim.lr_scheduler.ExponentialLR(optimizer_head, gamma=0.8)\n",
    "\n",
    "# optimizer_full = torch.optim.Adam([\n",
    "#     {'params': model.deeplab.backbone.parameters(), 'lr': LR},   \n",
    "#     {'params': model.deeplab.classifier.parameters(), 'lr': LR},\n",
    "# ])\n",
    "# scheduler_full = torch.optim.lr_scheduler.ExponentialLR(optimizer_full, gamma=0.8)\n",
    "\n",
    "# early_stopping = EarlyStopping(patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers and Schedules\n",
    "The commented out version is an older and worse version we used. It often leads the val_loss to stuck on a plateau. AdamW has built-in weight decay which handles regularization smartly. And ReduceLROnPlateau smartly decides when to reduce the lr, instead of reducing it continuously that sometimes leads to a too small lr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.backbone.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for m in model.backbone.modules():\n",
    "#     if isinstance(m, nn.BatchNorm2d):\n",
    "#         m.eval()\n",
    "\n",
    "# head_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "# optimizer_head = torch.optim.Adam([\n",
    "#     {'params': head_params, 'lr': LR_FROZEN},\n",
    "# ], lr=LR_FROZEN)\n",
    "# scheduler_head = torch.optim.lr_scheduler.ExponentialLR(optimizer_head, gamma=0.8)\n",
    "\n",
    "# optimizer_full = torch.optim.Adam([\n",
    "#     {'params': model.backbone.parameters(), 'lr': LR},   \n",
    "#     {'params': model.classifier.parameters(), 'lr': LR},\n",
    "# ])\n",
    "# scheduler_full = torch.optim.lr_scheduler.ExponentialLR(optimizer_full, gamma=0.8)\n",
    "\n",
    "# early_stopping = EarlyStopping(patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used for calculating weights for weighted CE but it isn't chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_enet_weights(dataset, num_classes, c=1.02, decimals=3):\n",
    "#     counts = np.zeros(num_classes, dtype=np.float64)\n",
    "#     total = 0\n",
    "#     for _, mask in dataset:\n",
    "#         m = np.array(mask)\n",
    "#         for cls in range(num_classes):\n",
    "#             counts[cls] += (m == cls).sum()\n",
    "#         total += m.size\n",
    "#     p = counts / total\n",
    "#     weights = 1.0 / np.log(c + p)\n",
    "#     return torch.tensor(np.round(weights, decimals), dtype=torch.float32)\n",
    "\n",
    "# class_weights = compute_enet_weights(train_dataset, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "for m in model.backbone.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.eval()\n",
    "\n",
    "head_param = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer_head = AdamW(\n",
    "    head_param,\n",
    "    lr=LR_FROZEN\n",
    ")\n",
    "scheduler_head = ReduceLROnPlateau(\n",
    "    optimizer_head,\n",
    "    mode='min',      \n",
    "    factor=0.8,     \n",
    "    patience=3,       \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "optimizer_full = AdamW([\n",
    "    {'params': model.backbone.parameters(), 'lr': LR},   \n",
    "    {'params': model.classifier.parameters(), 'lr': LR},\n",
    "])\n",
    "scheduler_full = ReduceLROnPlateau(\n",
    "    optimizer_full,\n",
    "    mode='min',\n",
    "    factor=0.8,\n",
    "    patience=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SegmentationTrainer(\n",
    "    model = model,  \n",
    "    optimizer_head = optimizer_head,\n",
    "    scheduler_head = scheduler_head,\n",
    "    criterion = criterion,\n",
    "    train_loader = train_dataloader, \n",
    "    val_loader = val_dataloader, \n",
    "    device = device,\n",
    "    early_stopping = early_stopping,\n",
    "    freeze_epoch = 5,\n",
    "    optimizer_full = optimizer_full,\n",
    "    scheduler_full = scheduler_full\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(EPOCH)\n",
    "trainer.visualize_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing\n",
    "In this section, two different post processing techniques are tried, which are dense conditional random field and a classfication<br>\n",
    "threshold, which acts the same as the threshold when calulating a ROC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCRF (Dense conditional random field)\n",
    "\n",
    "A DenseCRF refines the noisy, per-pixel label scores produced by a segmentation network by defining a global energy that combines those unary predictions with pairwise terms encouraging pixels that are both close in space and similar in color to share the same label. Inference is performed approximately via a mean-field algorithm that iteratively updates each pixels label distribution based on the entire image, using efficient high-dimensional filtering to propagate information in linear time. After a handful of iterations, the result is a segmentation with sharply defined boundaries and minimal isolated errors, fully aligned with the images natural edges.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_refine_probs(probs: Tensor, img: Tensor, n_iters: int = 5):\n",
    "    probs = probs.detach().cpu().numpy()\n",
    "    img = img.detach().cpu()\n",
    "\n",
    "    C, H, W = probs.shape\n",
    "    U = unary_from_softmax(probs)\n",
    "    d = dcrf.DenseCRF2D(W, H, C)\n",
    "    d.setUnaryEnergy(U)\n",
    "    \n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])[:, None, None]\n",
    "    std  = torch.tensor([0.229, 0.224, 0.225])[:, None, None]\n",
    "\n",
    "    img_unnorm = img * std + mean  \n",
    "\n",
    "    img_uint8 = (img_unnorm.clamp(0,1) * 255).byte()  \n",
    "    img_np = img_uint8.permute(1, 2, 0).numpy()  \n",
    "\n",
    "    feats = create_pairwise_bilateral(\n",
    "        sdims=(20, 20), schan=(13,13,13),\n",
    "        img=img_np, chdim=2\n",
    "    )\n",
    "    d.addPairwiseEnergy(feats, compat=21)\n",
    "\n",
    "    Q = d.inference(n_iters)                     \n",
    "\n",
    "    refined_probs = np.array(Q).reshape((C, H, W))\n",
    "    return refined_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(probs_batch, imgs_batch) -> Tensor:\n",
    "    processed_probs_batch = []\n",
    "    for (probs, img) in zip(probs_batch, imgs_batch):\n",
    "        processed_probs = crf_refine_probs(probs, img)\n",
    "        processed_probs_batch.append(processed_probs)\n",
    "\n",
    "    processed_probs_batch = torch.tensor(processed_probs_batch, dtype=torch.float32)\n",
    "    return processed_probs_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Mask\n",
    "The binary mask in the following applies different classification thresholds for all the pixels all the lables. For all pixels, it will only be considered as a label candidate if the probability is larger than the threshold. Then the final prediction is made as the label candidate with the highest probability. This increases DICE score about 0.004 in validation. Each label class has its own threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_thresholds_single_label(probs: torch.Tensor, thresholds: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compare all label channels, and channels with predicted probability < threshold are ignored.\n",
    "    Returns a (B, H, W) integer tensor of class indices.\n",
    "    \"\"\"\n",
    "    B, C, H, W = probs.shape\n",
    "    th = thresholds.view(1, -1, 1, 1).to(device)\n",
    "    mask = probs > th                        \n",
    "\n",
    "    masked_probs = probs.clone()\n",
    "    masked_probs[~mask] = -1.0              \n",
    "    idx = masked_probs.argmax(dim=1)       \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculates DICE score for a single class for fine-tuning the per class thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_single_channel(pred_mask: torch.Tensor, gt_mask: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    pred_f = pred_mask.view(pred_mask.size(0), -1).float()\n",
    "    gt_f = gt_mask.view(gt_mask.size(0), -1).float()\n",
    "    inter = (pred_f * gt_f).sum(dim=1)\n",
    "    denom = pred_f.sum(dim=1) + gt_f.sum(dim=1)\n",
    "    dice = (2 * inter + eps) / (denom + eps)\n",
    "    return dice.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for finetuning thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_thresholds(preds: torch.Tensor, target: torch.Tensor, num_steps: int = 100) -> torch.Tensor:\n",
    "    B, C, H, W = preds.shape\n",
    "    target_onehot = torch.stack([(target == c).to(torch.uint8) for c in range(C)], dim=1) # (B, H, W) to (B, C, H, W)\n",
    "\n",
    "    thresholds = torch.zeros(C, device=preds.device)\n",
    "    grid = torch.linspace(0, 1, num_steps, device=preds.device)\n",
    "\n",
    "    for c in trange(C, desc=\"Tuning thresholds for each class\"):\n",
    "        best_dice = -1.0\n",
    "        best_thres = 0.0\n",
    "        target_c = target_onehot[:, c] # (B, H, W)\n",
    "        pred_c = preds[:, c] # (B, H, W)\n",
    "        for thres in grid:\n",
    "            prediction = (pred_c > thres).to(torch.uint8)\n",
    "            dice_score  = dice_single_channel(prediction, target_c)\n",
    "            if dice_score > best_dice:\n",
    "                best_dice = dice_score\n",
    "                best_thres = thres\n",
    "        thresholds[c] = best_thres\n",
    "    \n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_probs = []\n",
    "all_targets = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with tqdm(val_dataloader, desc=f\"Validating\", unit=\"batch\") as pbar:\n",
    "        for images, masks in pbar:    \n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)               \n",
    "            logits = model(images)['out']                   \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            refined_probs = post_process(probs, images)\n",
    "            refined_probs = refined_probs.to(device)\n",
    "            all_probs.append(probs.cpu())\n",
    "            all_targets.append(masks.cpu())\n",
    "\n",
    "all_probs = torch.cat(all_probs, dim=0)        \n",
    "all_targets = torch.cat(all_targets, dim=0)        \n",
    "best_thresholds = tune_thresholds(all_probs, all_targets, num_steps=100)\n",
    "\n",
    "print(f'Tuned thresholds: {best_thresholds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DICE_evaluator(model, val_dataloader, device):\n",
    "    dice_loss = DiceLoss(smooth=1)\n",
    "    \n",
    "    voc_classes = [\n",
    "        \"background\",\n",
    "        \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
    "        \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "        \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
    "        \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\",\n",
    "    ]\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_loss_per_class = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(val_dataloader, desc=f\"Validating\", unit=\"batch\") as pbar:\n",
    "            for images, masks in pbar:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                logits = model(images)['out'] # Tensor [B, C, H, W]\n",
    "                probs  = F.softmax(logits, dim=1) # Tensor [B, C, H, W]\n",
    "                refined_probs = post_process(probs, images)\n",
    "                refined_probs = refined_probs.to(device)\n",
    "                preds = apply_thresholds_single_label(refined_probs, best_thresholds)\n",
    "                # preds = probs.argmax(dim=1)\n",
    "                preds_onehot = F.one_hot(preds, num_classes=21)   # [B, H, W, C]\n",
    "                preds_onehot = preds_onehot.permute(0, 3, 1, 2).float()\n",
    "                loss, loss_per_class = dice_loss(preds_onehot, masks)\n",
    "                total_loss += loss.item()\n",
    "                total_loss_per_class += loss_per_class\n",
    "\n",
    "                num_batches += 1\n",
    "        \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_loss_per_class = total_loss_per_class / num_batches\n",
    "\n",
    "    avg_loss_per_class_dict = {}\n",
    "    for name, score in zip(voc_classes, avg_loss_per_class):\n",
    "        avg_loss_per_class_dict[name] = float(score.cpu().numpy())\n",
    "\n",
    "    print(f'Final average DICE score: {1 - avg_loss}, \\n average DICE score per class:\\n {avg_loss_per_class_dict}')\n",
    "\n",
    "    return avg_loss_per_class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict = DICE_evaluator(trainer.model, val_dataloader, device)\n",
    "classes = list(loss_dict.keys())\n",
    "losses  = list(loss_dict.values())\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "bars = plt.bar(classes, losses)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Dice Loss\")\n",
    "plt.title(\"Per-Class Dice Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmentation(image: Tensor, mask: Tensor, pred: Tensor):\n",
    "    # Convert tensors to NumPy\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()  # Convert to HWC\n",
    "    mask = mask.cpu().numpy()\n",
    "    pred = pred.cpu().numpy()\n",
    "    \n",
    "    # Ensure mask and pred are 2D (H, W)\n",
    "    if mask.ndim > 2:\n",
    "        mask = mask.squeeze()\n",
    "    if pred.ndim > 2:\n",
    "        pred = pred.squeeze()\n",
    "    \n",
    "    # Initialize RGB images for masks\n",
    "    height, width = mask.shape\n",
    "    colored_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    colored_pred = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Get the viridis colormap\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    norm = mcolors.Normalize(vmin=0, vmax=20)  # Scale for labels [0, 20]\n",
    "    \n",
    "    # Map class indices to colors for ground truth and prediction\n",
    "    for class_idx in np.unique(np.concatenate([mask, pred])):\n",
    "        if class_idx <= 20:\n",
    "            # Convert normalized colormap value to RGB (0-255)\n",
    "            color = cmap(norm(class_idx))[:3]  # Get RGB (ignore alpha)\n",
    "            color = (np.array(color) * 255).astype(np.uint8)\n",
    "            colored_mask[mask == class_idx] = color\n",
    "            colored_pred[pred == class_idx] = color\n",
    "        elif class_idx == 255:\n",
    "            # Void label mapped to white, consistent with original visualize_segmentation\n",
    "            colored_mask[mask == class_idx] = (255, 255, 255)\n",
    "            colored_pred[pred == class_idx] = (255, 255, 255)\n",
    "        else:\n",
    "            print(f\"Warning: Class index {class_idx} not in expected range [0, 20] or 255. Using black.\")\n",
    "            colored_mask[mask == class_idx] = (0, 0, 0)\n",
    "            colored_pred[pred == class_idx] = (0, 0, 0)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.imshow(image)  # May need denormalization if normalized\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.imshow(colored_mask)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.imshow(colored_pred)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Return colored masks as PIL Images\n",
    "    return Image.fromarray(colored_mask), Image.fromarray(colored_pred)\n",
    "\n",
    "images, masks = next(iter(val_dataloader))\n",
    "images = images.to(device)\n",
    "masks = masks.to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(images)['out'] # Tensor [B, C, H, W]\n",
    "    probs  = F.softmax(logits, dim=1) # Tensor [B, C, H, W]\n",
    "    refined_probs = post_process(probs, images)\n",
    "    preds = torch.argmax(refined_probs, dim=1)\n",
    "    preds = torch.where(preds == 21, 255, preds)\n",
    "\n",
    "visualize_segmentation(images[1], masks[1], preds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestDataset\n",
    "Customized Testdataset.<br>\n",
    "It returns with original images as well, which is used for crf postprocessing for test instances. There is no augmentation applied for test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2009TestDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, target_transform=None, paired_transform=None, ignore_label=21):\n",
    "        self.df = dataframe.reset_index()\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.paired_transform = paired_transform\n",
    "        \n",
    "        self.ignore_label = ignore_label\n",
    "        self.classes = 22  # 20 classes + background + void\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[Tensor, Tensor]:\n",
    "        image = self.df.iloc[idx]['img'] # np.array\n",
    "        ori_img = image.copy().transpose((2, 0, 1))\n",
    "        mask = self.df.iloc[idx]['seg']   \n",
    "\n",
    "        image = Image.fromarray(image.astype(np.uint8))  \n",
    "        mask = Image.fromarray(mask.astype(np.uint8))    \n",
    "\n",
    "        if self.paired_transform:\n",
    "            image, mask = self.paired_transform(image, mask)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask, ori_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = VOC2009TestDataset(\n",
    "    dataframe=test_df,\n",
    "    transform=image_transform,\n",
    "    target_transform=mask_transform,\n",
    "    paired_transform=apply_paired_transform\n",
    "    )\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(test_dataloader, desc=f\"Predicting\", unit=\"batch\") as pbar:\n",
    "        for image, mask, ori_img in pbar: # tensors [1, C, H, W], [1, H, W], [1, C, H, W]\n",
    "            image = image.to(device)\n",
    "\n",
    "            logits = model(image)['out'] # Tensor [1, C, H, W]\n",
    "            logits_orig = F.interpolate(\n",
    "                logits, size=(ori_img.shape[2], ori_img.shape[3]),\n",
    "                mode='bilinear', align_corners=False\n",
    "            ) # [1, C, H, W]\n",
    "\n",
    "            probs_orig = F.softmax(logits_orig, dim=1)   # [1, C, H, W]\n",
    "            refined = post_process(\n",
    "                probs_orig.cpu(), # [1, C, H, W]\n",
    "                ori_img # [1, C, H, W]                        \n",
    "            )\n",
    "\n",
    "            pred = apply_thresholds_single_label(probs_orig, best_thresholds).squeeze(0)\n",
    "            pred = pred.to(torch.uint8).cpu().numpy()\n",
    "            pred = np.where(pred == 21, 255, pred).astype(np.uint8)\n",
    "            preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(pred, Tensor):\n",
    "    pred = torch.from_numpy(pred)\n",
    "\n",
    "visualize_segmentation(ori_img.squeeze(0), pred, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"seg\"] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to competition\n",
    "You don't need to edit this section. Just use it at the right position in the notebook. See the definition of this function in Sect. 1.3 for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_submission(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adversarial attack\n",
    "For this part, your goal is to fool your classification and/or segmentation CNN, using an *adversarial attack*. More specifically, the goal is build a CNN to perturb test images in a way that (i) they look unperturbed to humans; but (ii) the CNN classifies/segments these images in line with the perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Discussion\n",
    "Finally, take some time to reflect on what you have learned during this assignment. Reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Segmentation\n",
    "In transfer learning of segmentation, we have learned various architectures before exploring them, including FCN, UNet, DeepLabV3, etc. We also practiced transfer learning, which is the mostly used training method. At first we forgot to freeze initial layers, resuling in a much worse result. We also found post processing techniques are helpful. DCRF helps make segmentation boundaries sharper, making results more trust-worthy. Two common scenarios of segmentation include autonomous driving, where segmentation of lanes, objects etc. helps operation system make decisions, and medical domain, like tooth segmentation given a oral CT scan image.\n",
    "\n",
    "When evaluating results, both over segmentation and under segmentaion are observed. Like for a electrical bike image there are multilabels predicted for the bike, and for an image of a woman sitting on a couch many areas are identified as background. The concept of over and under segmentation is introduced in Lecture6. The (D)CRF algorithm is introduced in Lecture7. Lectures10 and 11 help us understand neural networks used here better. And specifically, (weighted) cross entropy and ResNet are introduced in Lecture11.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11236659,
     "sourceId": 94526,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
