{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94d258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb05512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional block, class satisfied earlier\n",
    "# MAPCallback definition (as before, needed for loading the x.keras model)\n",
    "class MAPCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, train_data, val_data):\n",
    "        super().__init__()\n",
    "        self.tr_ds = train_data\n",
    "        self.val_ds = val_data\n",
    "\n",
    "    def _compute_map(self, ds):\n",
    "        y_true, y_prob = [], []\n",
    "        for bx, by in ds:\n",
    "            # Ensure bx is a single tensor or list of tensors usable by model.predict\n",
    "            # In this case, bx is likely a batch of images, so it's fine.\n",
    "            y_prob.append(self.model.predict(bx, verbose=0))\n",
    "            # Convert TensorFlow Eager Tensor to NumPy array for y_true\n",
    "            if isinstance(by, tf.Tensor):\n",
    "                y_true.append(by.numpy())\n",
    "            else:  # Handle other potential data types if necessary\n",
    "                y_true.append(by)\n",
    "        # Check if y_true and y_prob have data before vstack and calculation\n",
    "        if not y_true or not y_prob:\n",
    "            print(\"Warning: No data to compute mAP for the given dataset.\")\n",
    "            return 0.0  # Or raise an error, depending on desired behavior\n",
    "\n",
    "        return average_precision_score(\n",
    "            np.vstack(y_true), np.vstack(y_prob), average=\"macro\"\n",
    "        )\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        # Note: _compute_map on large datasets inside epoch end can be slow\n",
    "        # and might not be the best practice depending on the use case.\n",
    "        # For this example, keeping it as in the original.\n",
    "        # Also, the attack training loop doesn't use this callback anyway.\n",
    "        logs[\"mAP\"] = self._compute_map(self.tr_ds)\n",
    "        logs[\"val_mAP\"] = self._compute_map(self.val_ds)\n",
    "        print(f\"\\ntrain mAP: {logs['mAP']:.4f}  \" f\"val mAP: {logs['val_mAP']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c120585b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained classifier model 'x.keras'...\n",
      "Classifier loaded successfully and set to non-trainable.\n"
     ]
    }
   ],
   "source": [
    "# Optional block, no need import model in production\n",
    "print(\"Loading pre-trained classifier model 'x.keras'...\")\n",
    "try:\n",
    "    classifier = tf.keras.models.load_model(\n",
    "        # '/kaggle/input/x/keras/default/1/x.keras',\n",
    "        'x.keras',\n",
    "        custom_objects={'MAPCallback': MAPCallback, 'BinaryFocalCrossentropy': tf.keras.losses.BinaryFocalCrossentropy}\n",
    "    )\n",
    "    # Set the classifier to non-trainable as it's our fixed target\n",
    "    classifier.trainable = False\n",
    "    print(\"Classifier loaded successfully and set to non-trainable.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading classifier model 'x.keras': {e}\")\n",
    "    # Assuming necessary for the rest of the script\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1e06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading true labels from kul-computer-vision-ga-2-2025/train/train_set.csv...\n",
      "Loaded true labels for 749 images.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration (Remains mostly the same, maybe structured as a dict/object later) ---\n",
    "# Optional, defined earlier or in other method\n",
    "labels = [\n",
    "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n",
    "    \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
    "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]\n",
    "# train_dir = '/kaggle/input/kul-computer-vision-ga-2-2025/train/'\n",
    "train_dir = 'kul-computer-vision-ga-2-2025/train/'\n",
    "image_dir = train_dir + 'img/'\n",
    "answer_fpath = train_dir + 'train_set.csv'\n",
    "\n",
    "# Optional, load true labels\n",
    "print(f\"Loading true labels from {answer_fpath}...\")\n",
    "try:\n",
    "    true_labels_df = pd.read_csv(answer_fpath)\n",
    "    true_labels_df.set_index('Id', inplace=True)\n",
    "    true_labels_df = true_labels_df[labels] # Ensure labels list is defined\n",
    "    print(f\"Loaded true labels for {len(true_labels_df)} images.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: True labels file not found at {answer_fpath}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading true labels CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "# fixed parameters\n",
    "TARGET_HEIGHT = 300\n",
    "TARGET_WIDTH = 300\n",
    "TARGET_CHANNELS = 3 # RGB3, gray-scale1\n",
    "\n",
    "\n",
    "# Adversarial specific config\n",
    "PREDICTION_THRESHOLD = 0.5\n",
    "EPSILON = 0.03      # Max pertubation (L-infinity norm)\n",
    "GENERATOR_LEARNING_RATE = 1e-4\n",
    "ADVERSARIAL_TRAINING_EPOCHS = 20 # Generator training epochs\n",
    "ADVERSARIAL_BATCH_SIZE = 16    # Generator training batch size\n",
    "DATASET_SHUFFLE_BUFFER = 1000 # A reasonable shuffle buffer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ff9e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialGenerator(tf.keras.Model):\n",
    "    def __init__(self, input_shape, num_channels_output, epsilon):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')\n",
    "        self.conv_output = tf.keras.layers.Conv2D(num_channels_output, (3, 3), padding='same', activation='tanh')\n",
    "        # Lambda layer to scale the tanh output by epsilon\n",
    "        # This layer scales the perturbation base [-1, 1] to [-epsilon, epsilon]\n",
    "        self.epsilon_scaler = tf.keras.layers.Lambda(lambda t: t * self.epsilon)\n",
    "\n",
    "        self.build((None,) + input_shape)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        perturbation_base = self.conv_output(x)\n",
    "        # Apply the epsilon scaling\n",
    "        perturbation = self.epsilon_scaler(perturbation_base)\n",
    "        return perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e8a5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional, preprocessor\n",
    "class ImageDataProcessor:\n",
    "    def __init__(self, image_dir, true_labels_df, labels, target_height, target_width, target_channels):\n",
    "        self.image_dir = image_dir\n",
    "        self.true_labels_df = true_labels_df\n",
    "        self.labels = labels\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "        self.target_channels = target_channels\n",
    "\n",
    "        # Internal lists to store processed data and corresponding IDs/paths\n",
    "        self._processed_images = None\n",
    "        self._processed_labels = None\n",
    "        self._processed_ids = None\n",
    "        self._relevant_files_info = None # Stores (id, path, label_array) tuples\n",
    "\n",
    "    def _find_relevant_files(self):\n",
    "        \"\"\"Finds image files that have corresponding true labels.\"\"\"\n",
    "        if self._relevant_files_info is not None:\n",
    "            return self._relevant_files_info # Return cached info if available\n",
    "\n",
    "        relevant_files_info = []\n",
    "        print(f\"Searching for relevant image files in {self.image_dir}...\")\n",
    "        # Iterate through files in the image directory\n",
    "        for filename in sorted(os.listdir(self.image_dir)):\n",
    "            # Check if filename matches the pattern train_ID.npy\n",
    "            if filename.startswith(\"train_\") and filename.endswith(\".npy\"):\n",
    "                # Extract ID\n",
    "                image_id_str = filename[len(\"train_\"):-len(\".npy\")]\n",
    "                try:\n",
    "                    image_id_int = int(image_id_str)\n",
    "                except ValueError:\n",
    "                    # Skip if ID is not a valid integer\n",
    "                    continue\n",
    "                # Check if the ID exists in the true labels DataFrame\n",
    "                if image_id_int in self.true_labels_df.index:\n",
    "                    file_path = os.path.join(self.image_dir, filename)\n",
    "                    # Get the corresponding true label array\n",
    "                    label_val = self.true_labels_df.loc[image_id_int, self.labels].values.astype(np.float32)\n",
    "                    relevant_files_info.append((image_id_int, file_path, label_val))\n",
    "\n",
    "        if not relevant_files_info:\n",
    "            print(\"Warning: No relevant image files with matching labels found.\")\n",
    "        else:\n",
    "             print(f\"Found {len(relevant_files_info)} relevant image files.\")\n",
    "\n",
    "        self._relevant_files_info = relevant_files_info # Cache the result\n",
    "        return relevant_files_info\n",
    "\n",
    "    def _load_and_preprocess_single_image(self, file_path):\n",
    "        \"\"\"Loads and preprocesses a single .npy image file.\"\"\"\n",
    "        try:\n",
    "            # Load numpy data from the file path\n",
    "            img_data = np.load(file_path)\n",
    "        except Exception as e:\n",
    "            # Print error and return None if loading fails\n",
    "            print(f\"Error loading .npy file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Handle potential extra batch dimension (shape [1, H, W, C])\n",
    "        if img_data.ndim == 4 and img_data.shape[0] == 1:\n",
    "            img_data = img_data[0]\n",
    "        # Check if the dimensions are correct (H, W, C)\n",
    "        elif img_data.ndim != 3:\n",
    "            print(f\"Skipping {file_path}: Unexpected data dimension {img_data.ndim}. Expected 3 (H,W,C) or 4 (1,H,W,C).\")\n",
    "            return None\n",
    "\n",
    "        # Convert to TensorFlow Tensor and resize to target dimensions\n",
    "        # Use method='area' or 'bilinear' depending on preference, 'area' is often good for downsampling\n",
    "        img_data_resized = tf.image.resize(img_data, [self.target_height, self.target_width], method='area')\n",
    "\n",
    "        # Handle channel mismatch if target_channels is specified\n",
    "        current_channels = img_data_resized.shape[-1]\n",
    "        if current_channels != self.target_channels:\n",
    "            # Convert grayscale (1 channel) to RGB (3 channels) if needed\n",
    "            if current_channels == 1 and self.target_channels == 3:\n",
    "                img_data_resized = tf.image.grayscale_to_rgb(img_data_resized)\n",
    "            # Convert RGB (3 channels) to grayscale (1 channel) if needed\n",
    "            elif self.target_channels == 1 and current_channels == 3:\n",
    "                 img_data_resized = tf.image.rgb_to_grayscale(img_data_resized)\n",
    "            else:\n",
    "                # Skip if channel conversion is not supported or possible\n",
    "                print(f\"Skipping {file_path}: Channel mismatch. Processed {current_channels} vs Target {self.target_channels}.\")\n",
    "                return None\n",
    "\n",
    "        # Cast to float32 type for consistency with model input\n",
    "        img_data_resized = tf.cast(img_data_resized, tf.float32)\n",
    "\n",
    "        # Normalize pixel values if they are in the 0-255 range\n",
    "        max_val = tf.reduce_max(img_data_resized)\n",
    "        # Check if max_val is significantly greater than 1.0\n",
    "        if max_val > 1.0 + 1e-6:\n",
    "            min_val = tf.reduce_min(img_data_resized)\n",
    "            # Assume 0-255 range if min is near 0 and max is near 255\n",
    "            if min_val >= 0 and max_val <= 255.0 + 1e-6 :\n",
    "                 img_data_resized = img_data_resized / 255.0\n",
    "            # Optional: Add warnings or other normalization logic for different ranges\n",
    "            # else:\n",
    "            #     print(f\"Warning: Image {file_path} has max value {max_val} but not clearly in 0-255 range. Check normalization.\")\n",
    "\n",
    "        # Clip values to the [0.0, 1.0] range after normalization\n",
    "        img_data_resized = tf.clip_by_value(img_data_resized, 0.0, 1.0)\n",
    "\n",
    "        # Return the processed image tensor\n",
    "        return img_data_resized\n",
    "\n",
    "    def process_all_relevant_data(self):\n",
    "        \"\"\"Loads and preprocesses all relevant images and stores them.\"\"\"\n",
    "        if self._processed_images is not None:\n",
    "            print(\"Using cached processed data.\")\n",
    "            # Return cached data\n",
    "            return self._processed_images, self._processed_labels, self._processed_ids\n",
    "\n",
    "        print(\"Starting to process all relevant images...\")\n",
    "        # Find the list of relevant files first\n",
    "        relevant_files = self._find_relevant_files()\n",
    "\n",
    "        # Initialize lists to store processed data\n",
    "        processed_images = []\n",
    "        processed_labels = []\n",
    "        processed_ids = []\n",
    "\n",
    "        # Iterate through the relevant files list\n",
    "        for image_id, file_path, label_val in relevant_files:\n",
    "            # Load and preprocess the single image\n",
    "            img = self._load_and_preprocess_single_image(file_path)\n",
    "            # Check if processing was successful and the shape matches the target\n",
    "            if img is not None and img.shape[-3:] == (self.target_height, self.target_width, self.target_channels):\n",
    "                # Append to the lists\n",
    "                processed_images.append(img)\n",
    "                processed_labels.append(label_val)\n",
    "                processed_ids.append(image_id)\n",
    "            elif img is not None:\n",
    "                 # Print warning for shape mismatch after processing\n",
    "                 print(f\"Skipping image {file_path} due to shape mismatch after processing: {img.shape} vs {(self.target_height, self.target_width, self.target_channels)}\")\n",
    "\n",
    "        # Check if any images were successfully processed\n",
    "        if not processed_images:\n",
    "            print(\"Error: No images could be successfully processed.\")\n",
    "            # Consider raising an exception here if no data is critical\n",
    "            self._processed_images, self._processed_labels, self._processed_ids = [], [], []\n",
    "            return [], [], [] # Return empty lists\n",
    "        else:\n",
    "            print(f\"Successfully processed {len(processed_images)} images.\")\n",
    "            # Store the processed data\n",
    "            # Convert lists of Tensors/Arrays to NumPy arrays for tf.data.Dataset.from_tensor_slices efficiency\n",
    "            self._processed_images = np.array(processed_images)\n",
    "            self._processed_labels = np.array(processed_labels)\n",
    "            self._processed_ids = processed_ids # Keep IDs as a list\n",
    "\n",
    "            return self._processed_images, self._processed_labels, self._processed_ids\n",
    "\n",
    "    def get_training_dataset(self, batch_size, shuffle_buffer_size=None):\n",
    "        \"\"\"Creates and returns a TensorFlow Dataset for training.\"\"\"\n",
    "        # Ensure data is processed before creating the dataset\n",
    "        images, labels, _ = self.process_all_relevant_data()\n",
    "\n",
    "        if not images.size: # Check if the numpy array is not empty\n",
    "             print(\"Cannot create dataset: No processed data available.\")\n",
    "             return None\n",
    "\n",
    "        # Create dataset from the processed NumPy arrays\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "        # Apply shuffling if buffer size is specified\n",
    "        if shuffle_buffer_size is not None:\n",
    "            dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "        # Apply batching and prefetching\n",
    "        dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79bbe811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialAttackExecutor:\n",
    "    def __init__(self, classifier_model, generator_model, optimizer_g, loss_fn, epsilon, prediction_threshold):\n",
    "        self.classifier = classifier_model\n",
    "        self.generator = generator_model\n",
    "        self.optimizer_g = optimizer_g\n",
    "        self.loss_fn = loss_fn # Should be a loss function compatible with classifier output and true labels\n",
    "        self.epsilon = epsilon # Store epsilon for reference, generator already uses it\n",
    "        self.prediction_threshold = prediction_threshold\n",
    "\n",
    "        # Ensure classifier is not trainable\n",
    "        self.classifier.trainable = False\n",
    "\n",
    "    def train_generator(self, train_dataset, epochs):\n",
    "        \"\"\"Trains the adversarial generator.\"\"\"\n",
    "        if train_dataset is None:\n",
    "            print(\"Error: Training dataset is not available. Cannot train generator.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\nStarting adversarial training for the generator...\")\n",
    "        # Iterate through training epochs\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            epoch_g_loss = 0.0 # Initialize total generator loss for this epoch\n",
    "            num_batches = 0   # Initialize batch counter\n",
    "\n",
    "            # Iterate through batches in the training dataset\n",
    "            for original_images_batch, true_labels_batch in train_dataset:\n",
    "                # Use tf.GradientTape to record operations for automatic differentiation\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Generate perturbations using the generator model\n",
    "                    # training=True is passed but might not change behavior for this generator structure\n",
    "                    perturbations = self.generator(original_images_batch, training=True)\n",
    "\n",
    "                    # Create adversarial images by adding perturbations to original images\n",
    "                    adversarial_images = original_images_batch + perturbations\n",
    "                    # Clip the adversarial images to the valid range [0.0, 1.0]\n",
    "                    # This ensures pixels stay within image boundaries\n",
    "                    adversarial_images = tf.clip_by_value(adversarial_images, 0.0, 1.0)\n",
    "\n",
    "                    # Get predictions from the fixed classifier on the adversarial images\n",
    "                    # training=False because the classifier is not being trained\n",
    "                    predictions_adversarial = self.classifier(adversarial_images, training=False)\n",
    "\n",
    "                    # Calculate the generator's loss\n",
    "                    # The generator wants to MAXIMIZE the classifier's loss\n",
    "                    # (i.e., make the classifier predict incorrectly)\n",
    "                    # So, the generator's loss is the NEGATIVE of the classifier's loss\n",
    "                    # This way, minimizing generator loss corresponds to maximizing classifier loss\n",
    "                    # Ensure predictions_adversarial and true_labels_batch are compatible with self.loss_fn\n",
    "                    adv_loss = -self.loss_fn(true_labels_batch, predictions_adversarial)\n",
    "\n",
    "                # Compute gradients of the adversarial loss with respect to the generator's trainable variables\n",
    "                gradients_g = tape.gradient(adv_loss, self.generator.trainable_variables)\n",
    "\n",
    "                # Apply the gradients to update the generator's weights\n",
    "                self.optimizer_g.apply_gradients(zip(gradients_g, self.generator.trainable_variables))\n",
    "\n",
    "                # Accumulate the batch loss\n",
    "                # Ensure adv_loss is a scalar or handle reduction correctly\n",
    "                epoch_g_loss += adv_loss.numpy()\n",
    "                num_batches += 1\n",
    "\n",
    "            # Calculate and print the average generator loss for the epoch\n",
    "            avg_epoch_g_loss = epoch_g_loss / num_batches if num_batches > 0 else 0\n",
    "            print(f\"  Generator loss: {avg_epoch_g_loss:.4f}\")\n",
    "\n",
    "        print(\"Finished adversarial training for the generator.\")\n",
    "\n",
    "    def generate_adversarial_predictions(self, relevant_files_info, data_processor_instance):\n",
    "        \"\"\"Generates adversarial predictions for a list of image files.\"\"\"\n",
    "        print(\"\\nGenerating adversarial examples and evaluating the original classifier on them...\")\n",
    "        predictions_dict_adversarial = {} # Dictionary to store predictions (binary labels) {id: [labels]}\n",
    "        processed_ids_adversarial = []   # List to store IDs for which predictions were generated\n",
    "\n",
    "        # Iterate through the list of relevant files (ID, path, label)\n",
    "        for image_id_int, file_path, _ in relevant_files_info: # We only need ID and path here\n",
    "\n",
    "            try:\n",
    "                # Load and preprocess the original image using the provided processor instance\n",
    "                # This reuses the preprocessing logic defined in ImageDataProcessor\n",
    "                original_img_processed = data_processor_instance._load_and_preprocess_single_image(file_path)\n",
    "                # Skip if preprocessing failed or returned None\n",
    "                if original_img_processed is None:\n",
    "                    continue\n",
    "\n",
    "                # Add a batch dimension to the processed image (H, W, C) -> (1, H, W, C)\n",
    "                original_img_batch = tf.expand_dims(original_img_processed, axis=0)\n",
    "                # Ensure it's float32, though preprocessing should handle this\n",
    "                original_img_batch = tf.cast(original_img_batch, tf.float32)\n",
    "\n",
    "                # Use the trained generator to predict the perturbation\n",
    "                # training=False in evaluation mode\n",
    "                perturbation_batch = self.generator(original_img_batch, training=False)\n",
    "\n",
    "                # Create the adversarial image\n",
    "                adversarial_image_batch = original_img_batch + perturbation_batch\n",
    "                # Clip values to ensure they are within the valid [0.0, 1.0] range\n",
    "                adversarial_image_batch = tf.clip_by_value(adversarial_image_batch, 0.0, 1.0)\n",
    "\n",
    "                # Get predictions from the classifier on the adversarial image\n",
    "                predictions_adv = self.classifier.predict(adversarial_image_batch, verbose=0)\n",
    "\n",
    "                # Check if the prediction output shape matches the number of labels\n",
    "                if predictions_adv.shape[-1] == len(labels): # Check last dim of output shape\n",
    "                    # Convert probability predictions to binary labels using the threshold\n",
    "                    predicted_binary_adv = (predictions_adv[0] > self.prediction_threshold).astype(int)\n",
    "                    # Store the prediction result in the dictionary\n",
    "                    predictions_dict_adversarial[image_id_int] = predicted_binary_adv\n",
    "                    # Add the image ID to the list of processed IDs\n",
    "                    processed_ids_adversarial.append(image_id_int)\n",
    "                # else:\n",
    "                #     print(f\"Skipping {file_path}: Classifier output shape {predictions_adv.shape} doesn't match expected label count.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Print error for any exception during processing or prediction\n",
    "                print(f\"Error processing {file_path} for adversarial evaluation: {e}\")\n",
    "\n",
    "        print(f\"Finished generating adversarial predictions for {len(processed_ids_adversarial)} files.\")\n",
    "        # Return the dictionary of predictions and the list of processed IDs\n",
    "        return predictions_dict_adversarial, processed_ids_adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a5a291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Setting up Adversarial Attack ---\n",
      "Starting to process all relevant images...\n",
      "Searching for relevant image files in kul-computer-vision-ga-2-2025/train/img/...\n",
      "Found 749 relevant image files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaoshangqi/Code/playground/cvProject/.venv/lib/python3.12/site-packages/keras/src/layers/layer.py:396: UserWarning: `build()` was called on layer 'adversarial_generator', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 749 images.\n",
      "\n",
      "Starting adversarial training for the generator...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 17:36:19.871882: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generator loss: -0.1329\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/var/folders/cl/qg7rfmvx3bzgwwh67m7_l9l80000gn/T/ipykernel_54488/556282335.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     54\u001b[39m     prediction_threshold=PREDICTION_THRESHOLD \u001b[38;5;66;03m# Pass the threshold for later evaluation\u001b[39;00m\n\u001b[32m     55\u001b[39m )\n\u001b[32m     56\u001b[39m \n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# 6. Train the Generator\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m attack_executor.train_generator(\n\u001b[32m     59\u001b[39m     train_dataset=train_dataset_gen,\n\u001b[32m     60\u001b[39m     epochs=ADVERSARIAL_TRAINING_EPOCHS\n\u001b[32m     61\u001b[39m )\n",
      "\u001b[32m/var/folders/cl/qg7rfmvx3bzgwwh67m7_l9l80000gn/T/ipykernel_54488/3588710355.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, train_dataset, epochs)\u001b[39m\n\u001b[32m     49\u001b[39m                     \u001b[38;5;66;03m# Ensure predictions_adversarial and true_labels_batch are compatible with self.loss_fn\u001b[39;00m\n\u001b[32m     50\u001b[39m                     adv_loss = -self.loss_fn(true_labels_batch, predictions_adversarial)\n\u001b[32m     51\u001b[39m \n\u001b[32m     52\u001b[39m                 \u001b[38;5;66;03m# Compute gradients of the adversarial loss with respect to the generator's trainable variables\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m                 gradients_g = tape.gradient(adv_loss, self.generator.trainable_variables)\n\u001b[32m     54\u001b[39m \n\u001b[32m     55\u001b[39m                 \u001b[38;5;66;03m# Apply the gradients to update the generator's weights\u001b[39;00m\n\u001b[32m     56\u001b[39m                 self.optimizer_g.apply_gradients(zip(gradients_g, self.generator.trainable_variables))\n",
      "\u001b[32m~/Code/playground/cvProject/.venv/lib/python3.12/site-packages/tensorflow/python/eager/backprop.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[39m\n\u001b[32m   1062\u001b[39m               output_gradients))\n\u001b[32m   1063\u001b[39m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[32m   1064\u001b[39m                           \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m output_gradients]\n\u001b[32m   1065\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m     flat_grad = imperative_grad.imperative_grad(\n\u001b[32m   1067\u001b[39m         self._tape,\n\u001b[32m   1068\u001b[39m         flat_targets,\n\u001b[32m   1069\u001b[39m         flat_sources,\n",
      "\u001b[32m~/Code/playground/cvProject/.venv/lib/python3.12/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[39m\n\u001b[32m     63\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m ValueError:\n\u001b[32m     64\u001b[39m     raise ValueError(\n\u001b[32m     65\u001b[39m         \u001b[33m\"Unknown value for unconnected_gradients: %r\"\u001b[39m % unconnected_gradients)\n\u001b[32m     66\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[32m     68\u001b[39m       tape._tape,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m     69\u001b[39m       target,\n\u001b[32m     70\u001b[39m       sources,\n",
      "\u001b[32m~/Code/playground/cvProject/.venv/lib/python3.12/site-packages/tensorflow/python/ops/custom_gradient.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*result_grad_components)\u001b[39m\n\u001b[32m    579\u001b[39m       \u001b[38;5;28;01mif\u001b[39;00m len(variable_grads) != len(variables):\n\u001b[32m    580\u001b[39m         raise ValueError(\"Must return gradient for each variable from \"\n\u001b[32m    581\u001b[39m                          \u001b[33m\"@custom_gradient grad_fn.\"\u001b[39m)\n\u001b[32m    582\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m       input_grads = grad_fn(*result_grads)\n\u001b[32m    584\u001b[39m       variable_grads = []\n\u001b[32m    585\u001b[39m     flat_grads = composite_tensor_gradient.get_flat_tensors_for_gradients(\n\u001b[32m    586\u001b[39m         nest.flatten(input_grads))\n",
      "\u001b[32m~/Code/playground/cvProject/.venv/lib/python3.12/site-packages/tensorflow/python/ops/nn_impl.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(dy)\u001b[39m\n\u001b[32m    471\u001b[39m         sigmoid_features = math_ops.sigmoid(beta * features)\n\u001b[32m    472\u001b[39m \n\u001b[32m    473\u001b[39m       activation_grad = (\n\u001b[32m    474\u001b[39m           sigmoid_features * (1.0 + (beta * features) *\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m                               (\u001b[32m1.0\u001b[39m - sigmoid_features)))\n\u001b[32m    476\u001b[39m       beta_grad = math_ops.reduce_sum(\n\u001b[32m    477\u001b[39m           dy * math_ops.square(features) * sigmoid_features *\n\u001b[32m    478\u001b[39m           (\u001b[32m1.0\u001b[39m - sigmoid_features))\n",
      "\u001b[32m~/Code/playground/cvProject/.venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~/Code/playground/cvProject/.venv/lib/python3.12/site-packages/tensorflow/python/framework/override_binary_operator.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(y, x)\u001b[39m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(\u001b[38;5;28;01mNone\u001b[39;00m, op_name, [x, y]) \u001b[38;5;28;01mas\u001b[39;00m name:\n\u001b[32m    147\u001b[39m       \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[32m    148\u001b[39m       \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[32m    149\u001b[39m       y, x = maybe_promote_tensors(y, x, force_same_dtype=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(x, y, name=name)\n",
      "\u001b[32m~/Code/playground/cvProject/.venv/lib/python3.12/site-packages/tensorflow/python/ops/tensor_math_operator_overrides.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m _subtract_factory(x, y, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     92\u001b[39m   \u001b[38;5;28;01mfrom\u001b[39;00m tensorflow.python.ops \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[32m     93\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m math_ops.subtract(x, y, name=name)\n",
      "\u001b[32m~/Code/playground/cvProject/.venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~/Code/playground/cvProject/.venv/lib/python3.12/site-packages/tensorflow/python/util/dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32m~/Code/playground/cvProject/.venv/lib/python3.12/site-packages/tensorflow/python/ops/math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m    541\u001b[39m @tf_export(\u001b[33m\"math.subtract\"\u001b[39m, \u001b[33m\"subtract\"\u001b[39m)\n\u001b[32m    542\u001b[39m @dispatch.register_binary_elementwise_api\n\u001b[32m    543\u001b[39m @dispatch.add_dispatch_support\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m subtract(x, y, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops.sub(x, y, name)\n",
      "\u001b[32m~/Code/playground/cvProject/.venv/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    140\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m op(*args, **kwargs)\n\u001b[32m    143\u001b[39m     bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m    144\u001b[39m     bound_arguments.apply_defaults()\n\u001b[32m    145\u001b[39m     bound_kwargs = bound_arguments.arguments\n",
      "\u001b[32m~/Code/playground/cvProject/.venv/lib/python3.12/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m  12304\u001b[39m         _ctx, \u001b[33m\"Sub\"\u001b[39m, name, x, y)\n\u001b[32m  12305\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m  12306\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m  12307\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m> \u001b[39m\u001b[32m12308\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m  12309\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m  12310\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m  12311\u001b[39m       return sub_eager_fallback(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Setting up Adversarial Attack ---\")\n",
    "\n",
    "# 1. Instantiate the Generator Model\n",
    "# Pass input shape and epsilon\n",
    "generator = AdversarialGenerator(\n",
    "    input_shape=(TARGET_HEIGHT, TARGET_WIDTH, TARGET_CHANNELS),\n",
    "    num_channels_output=TARGET_CHANNELS,\n",
    "    epsilon=EPSILON\n",
    ")\n",
    "\n",
    "# 2. Define Optimizer and Loss for Generator Training\n",
    "optimizer_g = tf.keras.optimizers.Adam(learning_rate=GENERATOR_LEARNING_RATE)\n",
    "# The loss function is the classifier's loss function\n",
    "# Note: In the original code, this was BinaryFocalCrossentropy.\n",
    "# Let's define it here, matching the one used by the classifier.\n",
    "# Ensure from_logits matches the classifier's output (False means sigmoid output)\n",
    "# The reduction strategy should match how you want to average loss over the batch\n",
    "# SUM_OVER_BATCH_SIZE means sum losses for all items in batch, then divide by batch size\n",
    "classifier_loss_fn = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "    from_logits=False,\n",
    "    reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Instantiate the Data Processor\n",
    "data_processor = ImageDataProcessor(\n",
    "    image_dir=image_dir,\n",
    "    true_labels_df=true_labels_df, # Pass the loaded dataframe\n",
    "    labels=labels,                 # Pass the labels list\n",
    "    target_height=TARGET_HEIGHT,\n",
    "    target_width=TARGET_WIDTH,\n",
    "    target_channels=TARGET_CHANNELS\n",
    ")\n",
    "\n",
    "# 4. Prepare the Training Dataset\n",
    "# This calls internal methods to find files, preprocess, and create the dataset\n",
    "train_dataset_gen = data_processor.get_training_dataset(\n",
    "    batch_size=ADVERSARIAL_BATCH_SIZE,\n",
    "    shuffle_buffer_size=DATASET_SHUFFLE_BUFFER # Using defined buffer size\n",
    ")\n",
    "\n",
    "# Check if dataset creation was successful\n",
    "if train_dataset_gen is None:\n",
    "    print(\"Failed to prepare training dataset. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# 5. Instantiate the Adversarial Attack Executor\n",
    "attack_executor = AdversarialAttackExecutor(\n",
    "    classifier_model=classifier,\n",
    "    generator_model=generator,\n",
    "    optimizer_g=optimizer_g,\n",
    "    loss_fn=classifier_loss_fn, # Pass the classifier's loss function\n",
    "    epsilon=EPSILON, # Pass epsilon (mostly for reference)\n",
    "    prediction_threshold=PREDICTION_THRESHOLD # Pass the threshold for later evaluation\n",
    ")\n",
    "\n",
    "# 6. Train the Generator\n",
    "attack_executor.train_generator(\n",
    "    train_dataset=train_dataset_gen,\n",
    "    epochs=ADVERSARIAL_TRAINING_EPOCHS\n",
    ")\n",
    "\n",
    "# 7. Generate Adversarial Predictions\n",
    "# Get the list of relevant files (IDs and paths) from the data processor\n",
    "relevant_files_for_pred = data_processor._find_relevant_files() # Use the internal list\n",
    "\n",
    "# Generate predictions on adversarial examples\n",
    "predictions_dict_adversarial, processed_ids_adversarial = attack_executor.generate_adversarial_predictions(\n",
    "    relevant_files_info=relevant_files_for_pred,\n",
    "    data_processor_instance=data_processor # Pass the processor instance for loading/preprocessing\n",
    ")\n",
    "\n",
    "# 8. Evaluate the Attack (Calculate accuracy on adversarial examples)\n",
    "# This part remains outside the executor class as it's a separate evaluation step\n",
    "# using the results generated by the executor and the original true labels.\n",
    "valid_processed_ids_adv = [\n",
    "    id_val for id_val in processed_ids_adversarial\n",
    "    if id_val in predictions_dict_adversarial and id_val in true_labels_df.index\n",
    "]\n",
    "\n",
    "if not valid_processed_ids_adv:\n",
    "    print(\"\\nNo valid adversarial predictions with matching true labels stored. Cannot calculate accuracy on adversarial examples.\")\n",
    "else:\n",
    "    print(f\"\\nCalculating accuracy on adversarial examples for {len(valid_processed_ids_adv)} images...\")\n",
    "    valid_processed_ids_adv.sort() # Sort IDs for consistent order\n",
    "\n",
    "    # Extract true labels and predicted labels for the valid IDs\n",
    "    true_matrices_list_adv = [true_labels_df.loc[id_val, labels].values for id_val in valid_processed_ids_adv]\n",
    "    predicted_matrices_list_adv = [predictions_dict_adversarial[id_val] for id_val in valid_processed_ids_adv]\n",
    "\n",
    "    true_matrix_adv = np.array(true_matrices_list_adv)\n",
    "    predicted_matrix_adv = np.array(predicted_matrices_list_adv)\n",
    "\n",
    "    # Perform shape and emptiness checks before calculating accuracy\n",
    "    if true_matrix_adv.shape != predicted_matrix_adv.shape:\n",
    "        print(f\"Error: Shape mismatch for adversarial data. True: {true_matrix_adv.shape}, Pred: {predicted_matrix_adv.shape}\")\n",
    "    elif true_matrix_adv.size == 0:\n",
    "        print(\"Error: Adversarial true/predicted matrices are empty after filtering.\")\n",
    "    else:\n",
    "        num_images_adv, num_labels_adv = true_matrix_adv.shape\n",
    "        print(f\"Comparing true and predicted (adversarial) labels for {num_images_adv} images and {num_labels_adv} classes.\")\n",
    "\n",
    "        print(\"\\n--- Per-Class Accuracy (Adversarial) ---\")\n",
    "        # Calculate and print per-class accuracy\n",
    "        for i, label_name in enumerate(labels):\n",
    "            true_col_adv = true_matrix_adv[:, i]\n",
    "            predicted_col_adv = predicted_matrix_adv[:, i]\n",
    "            # Accuracy for a class is (TP + TN) / Total Samples\n",
    "            correct_predictions_for_class_adv = (true_col_adv == predicted_col_adv).sum()\n",
    "            accuracy_adv = correct_predictions_for_class_adv / num_images_adv if num_images_adv > 0 else 0\n",
    "            print(f\"Accuracy for '{label_name}' (adversarial): {accuracy_adv:.4f}\")\n",
    "\n",
    "        print(\"\\n--- Overall (Micro) Accuracy (Adversarial) ---\")\n",
    "        # Calculate and print overall micro accuracy (accuracy across all individual label predictions)\n",
    "        total_correct_predictions_adv = (true_matrix_adv == predicted_matrix_adv).sum()\n",
    "        total_possible_predictions_adv = float(true_matrix_adv.size)\n",
    "        overall_accuracy_adv = total_correct_predictions_adv / total_possible_predictions_adv if total_possible_predictions_adv > 0 else 0\n",
    "        print(f\"Overall (Micro) Accuracy (Adversarial): {overall_accuracy_adv:.4f}\")\n",
    "\n",
    "print(\"\\n--- Adversarial Attack Block Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_visualizations = 5\n",
    "\n",
    "# Ensure there are valid processed IDs to visualize\n",
    "if not valid_processed_ids_adv:\n",
    "    print(\"No valid adversarial examples generated to visualize.\")\n",
    "else:\n",
    "    # Select a few random IDs from the list of successfully processed adversarial examples\n",
    "    # Using np.random.choice ensures uniqueness and handles cases where num_visualizations > list length\n",
    "    ids_to_visualize = np.random.choice(valid_processed_ids_adv,\n",
    "                                        min(num_visualizations, len(valid_processed_ids_adv)),\n",
    "                                        replace=False)\n",
    "\n",
    "    print(f\"Visualizing {len(ids_to_visualize)} random examples...\")\n",
    "\n",
    "    # Get the list of relevant files info from the data processor (it's cached after processing)\n",
    "    relevant_files_info = data_processor._find_relevant_files()\n",
    "    # Create a dictionary mapping ID to (path, label) for quick lookup\n",
    "    relevant_files_dict = {id_: (path, label) for id_, path, label in relevant_files_info}\n",
    "\n",
    "    # Iterate through the selected IDs\n",
    "    for image_id in ids_to_visualize:\n",
    "        print(f\"\\nVisualizing Image ID: {image_id}\")\n",
    "\n",
    "        # Get file path and true label for this ID\n",
    "        file_path, true_label_array = relevant_files_dict.get(image_id)\n",
    "        if file_path is None:\n",
    "             print(f\"Could not find file info for ID {image_id}, skipping.\")\n",
    "             continue\n",
    "\n",
    "        # Get the adversarial prediction for this ID\n",
    "        adv_prediction_array = predictions_dict_adversarial.get(image_id)\n",
    "        if adv_prediction_array is None:\n",
    "             print(f\"Could not find adversarial prediction for ID {image_id}, skipping.\")\n",
    "             continue\n",
    "\n",
    "        try:\n",
    "            # Load and preprocess the original image using the processor instance\n",
    "            original_img_processed = data_processor._load_and_preprocess_single_image(file_path)\n",
    "\n",
    "            if original_img_processed is None:\n",
    "                print(f\"Preprocessing failed for ID {image_id}, skipping visualization.\")\n",
    "                continue\n",
    "\n",
    "            # Ensure the image is in the expected float32 format and has the correct shape\n",
    "            if not isinstance(original_img_processed, tf.Tensor) or original_img_processed.dtype != tf.float32:\n",
    "                 original_img_processed = tf.cast(original_img_processed, tf.float32)\n",
    "\n",
    "            # Generate adversarial image (using the trained generator)\n",
    "            # Add batch dimension (H, W, C) -> (1, H, W, C)\n",
    "            original_img_batch = tf.expand_dims(original_img_processed, axis=0)\n",
    "\n",
    "            # Use the trained generator from the attack executor\n",
    "            perturbation_batch = attack_executor.generator(original_img_batch, training=False)\n",
    "\n",
    "            # Create adversarial image\n",
    "            adversarial_image_batch = original_img_batch + perturbation_batch\n",
    "            # Clip values back to [0.0, 1.0]\n",
    "            adversarial_image_batch = tf.clip_by_value(adversarial_image_batch, 0.0, 1.0)\n",
    "\n",
    "            # Convert Tensors to NumPy arrays for plotting and remove batch dimension\n",
    "            original_img_np = tf.squeeze(original_img_batch, axis=0).numpy()\n",
    "            adversarial_img_np = tf.squeeze(adversarial_image_batch, axis=0).numpy()\n",
    "            perturbation_np = tf.squeeze(perturbation_batch, axis=0).numpy() # Optional: visualize perturbation too\n",
    "\n",
    "            # --- Plotting ---\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 6)) # 1 row, 2 columns\n",
    "\n",
    "            # Plot Original Image\n",
    "            axes[0].imshow(original_img_np)\n",
    "            axes[0].set_title(\"Original Image\")\n",
    "            axes[0].axis('off') # Hide axes ticks and labels\n",
    "\n",
    "            # Plot Adversarial Image\n",
    "            axes[1].imshow(adversarial_img_np)\n",
    "            axes[1].set_title(\"Adversarial Image\")\n",
    "            axes[1].axis('off') # Hide axes ticks and labels\n",
    "\n",
    "            # --- Add Title with Labels/Predictions ---\n",
    "            # Format true labels and adversarial predictions\n",
    "            true_labels_str = [labels[i] for i, val in enumerate(true_label_array) if val == 1]\n",
    "            adv_pred_str = [labels[i] for i, val in enumerate(adv_prediction_array) if val == 1]\n",
    "\n",
    "            # Compare and highlight prediction changes\n",
    "            comparison_str = []\n",
    "            for i, label_name in enumerate(labels):\n",
    "                true_val = true_label_array[i]\n",
    "                pred_val = adv_prediction_array[i]\n",
    "                if true_val == 1 and pred_val == 1:\n",
    "                    comparison_str.append(f\"{label_name}\") # Correctly predicted positive\n",
    "                elif true_val == 1 and pred_val == 0:\n",
    "                    comparison_str.append(f\"{label_name}\") # Missed positive\n",
    "                elif true_val == 0 and pred_val == 1:\n",
    "                     comparison_str.append(f\"{label_name}\") # False positive (attack success for this label)\n",
    "                # If true_val is 0 and pred_val is 0, it's a correct negative, usually less interesting for attack visualization\n",
    "                # Optionally include: elif true_val == 0 and pred_val == 0: comparison_str.append(f\"{label_name}\")\n",
    "\n",
    "            main_title = f\"ID: {image_id} | True: [{', '.join(true_labels_str)}] | Adv Pred (Threshold={PREDICTION_THRESHOLD}): [{', '.join(adv_pred_str)}]\\nComparison: [{', '.join(comparison_str)}]\"\n",
    "            fig.suptitle(main_title, y=1.02) # Add a super title slightly above the plots\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust layout to prevent title overlap\n",
    "            plt.show() # Display the figure\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during visualization for ID {image_id}: {e}\")\n",
    "            # Continue to the next image even if one fails"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
